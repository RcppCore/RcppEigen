\documentclass[10pt]{article}
%\VignetteIndexEntry{RcppEigen-intro}
\usepackage[margin=1in]{geometry}
\usepackage{color, alltt, bm, amsmath}%, listings}
%\lstset{language=C++,basicstyle=\small}
\usepackage[authoryear,round,longnamesfirst]{natbib}
\usepackage{booktabs,flafter,thumbpdf} % booktabs for nice tables, flafter for float control, thumbpdf is a tip from Achim
\usepackage[colorlinks]{hyperref}
\definecolor{link}{rgb}{0,0,0.3}	%% next few lines courtesy of RJournal.sty
\hypersetup{
    colorlinks,%
    citecolor=link,%
    filecolor=link,%
    linkcolor=link,%
    urlcolor=link
}
\newcommand{\proglang}[1]{\textsf{#1}}
\newcommand{\pkg}[1]{{\fontseries{b}\selectfont #1}}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\rank}{\operatorname{rank}}

%% highlights macros
%% Style definition file generated by highlight 2.7, http://www.andre-simon.de/
% Highlighting theme definition:
\newcommand{\hlstd}[1]{\textcolor[rgb]{0,0,0}{#1}}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0,0,0}{#1}}
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}
\newcommand{\hlesc}[1]{\textcolor[rgb]{0.74,0.55,0.55}{#1}}
%\newcommand{\hlstr}[1]{\textcolor[rgb]{0.74,0.55,0.55}{#1}}
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.90,0.15,0.15}{#1}}
%green: \newcommand{\hlstr}[1]{\textcolor[rgb]{0.13,0.67,0.13}{#1}} % 0.74 -> % 0.90; 0.55 -> 0.25
\newcommand{\hldstr}[1]{\textcolor[rgb]{0.74,0.55,0.55}{#1}}
\newcommand{\hlslc}[1]{\textcolor[rgb]{0.67,0.13,0.13}{\it{#1}}}
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.67,0.13,0.13}{\it{#1}}}
\newcommand{\hldir}[1]{\textcolor[rgb]{0,0,0}{#1}}
\newcommand{\hlsym}[1]{\textcolor[rgb]{0,0,0}{#1}}
\newcommand{\hlline}[1]{\textcolor[rgb]{0.33,0.33,0.33}{#1}}
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.61,0.13,0.93}{\bf{#1}}}
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.13,0.54,0.13}{#1}}
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0,0,1}{#1}}
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0,0,0}{#1}}
\definecolor{bgcolor}{rgb}{1,1,1}


<<version,echo=FALSE,print=FALSE>>=
prettyVersion <- packageDescription("RcppEigen")$Version
prettyDate <- format(Sys.Date(), "%B %e, %Y")
options(prompt="> ")  # to overcome DE's default of "R> "
@
\author{Douglas Bates \and Dirk Eddelbuettel \and Romain Fran\c{c}ois}
\title{An Introduction to \pkg{RcppEigen}}
\date{\pkg{RcppEigen} version \Sexpr{prettyVersion} as of \Sexpr{prettyDate}}

<<preliminaries,echo=FALSE>>=
link <- function( f, package, text = f, root = "http://finzi.psych.upenn.edu/R/library/" ){
    h <- if( missing(package) ) {
        as.character( help( f ) )
    } else {
        as.character( help( f, package = paste( package, sep = "" ) ) )
    }
    if( ! length(h) ){
        sprintf( "\\\\textbf{%s}", f )
    } else {
        rx <- "^.*/([^/]*?)/help/(.*?)$"
        package <- sub( rx, "\\1", h, perl = TRUE )
        page <- sub( rx, "\\2", h, perl = TRUE )
        sprintf( "\\\\href{%s%s/html/%s.html}{\\\\texttt{%s}}", root, package, page, text )
    }
}
linkS4class <- function( cl, package, text = cl, root = "http://finzi.psych.upenn.edu/R/library/" ){
    link( sprintf("%s-class", cl), package, text, root )
}
require( inline )
require( RcppEigen )
@
\begin{document}

%% Below is what DE currently uses somewhere else -- I am not religuous about
%% the grey background etc -- comment out and see if you like any of it
%% Just like DB below, DE also uses  frame=tb  for top+bottom frame lines
\definecolor{lightgray}{rgb}{0.975,0.975,0.975}
%\lstset{backgroundcolor=\color{lightgray}}
% \lstset{numbers=left, numberstyle=\tiny, stepnumber=2, numbersep=5pt}
%\lstset{keywordstyle=\color{black}\bfseries\tt}
% \lstset{ %
%   %language=Octave,                % the language of the code
%   %basicstyle=\footnotesize,       % the size of the fonts that are used for the code
%   basicstyle=\small,              % the size of the fonts that are used for the code
%   numbers=left,                   % where to put the line-numbers
%   %numberstyle=\footnotesize,      % the size of the fonts that are used for the line-numbers
%   stepnumber=2,                   % the step between two line-numbers. If it's 1, each line
%                                   % will be numbered
%   numbersep=5pt,                  % how far the line-numbers are from the code
%   %backgroundcolor=\color{white},  % choose the background color. You must add \usepackage{color}
%   showspaces=false,               % show spaces adding particular underscores
%   showstringspaces=false,         % underline spaces within strings
%   showtabs=false,                 % show tabs within strings adding particular underscores
%   %frame=single,                   % adds a frame around the code
%   tabsize=2,                      % sets default tabsize to 2 spaces
%   captionpos=b,                   % sets the caption-position to bottom
%   breaklines=true,                % sets automatic line breaking
%   breakatwhitespace=false         % sets if automatic breaks should only happen at whitespace
%   %title=\lstname,                 % show the filename of files included with \lstinputlisting;
%                                   % also try caption instead of title
%   %escapeinside={\%*}{*)},         % if you want to add a comment within your code
%   %morekeywords={*,...}            % if you want to add more keywords to the set
% }


\maketitle

\abstract{
  \noindent
  The \pkg{RcppEigen} package provides access from \proglang{R}
  \citep{R:Main} to the \pkg{Eigen} \citep*{Eigen:Web} \proglang{C++} template library for
  numerical linear algebra. \pkg{Rcpp} \citep{JSS:Rcpp,CRAN:Rcpp} classes and
  specializations of the \proglang{C++} templated functions \code{as} and
  \code{wrap} from \pkg{Rcpp} provide the ``glue'' for passing objects from
  \proglang{R} to \proglang{C++} and back.
}

\section{Introduction}
\label{sec:intro}

Linear algebra is an essential building block of statistical computing.
Operations such as matrix decompositions, linear program solvers, and
eigenvalue / eigenvector computations are used in many estimation and
analysis routines. As such, libraries supporting linear algebra have long been
provided by statistical programmers for different programming languages and
environments. \proglang{C++}, one of the central modern languages for numerical
and statistical computing, can be extended particularly well due to its
object-oriented nature, and numerous class libraries providing linear algebra
routines have been written over the years.
% Could cite Eddelbuettel (1996) here, but a real survey would be better.

As both the \proglang{C++} language and standards have evolved
\citep{Meyers:2005:EffectiveC++,Meyers:1995:MoreEffectiveC++}, so have the
compilers implementing the language.  Relatively modern language constructs
such as template meta-programming are particularly useful. It provides both
overloading of operations (allowing expressive code in the compiled language
similar to what can be done in scripting languages) and can shift some of the
computational burden from the run-time to the compile-time (though a more
detailed discussions of template programming is however beyond this
paper). \cite{Veldhuizen:1998:Blitz} provided an early and influential
implementation that already demonstrated key features of this approach.  Its
usage however was held back at the time by the somewhat limited availability
of compilers implementing all necessary features of the \proglang{C++}
language.

This situation has greatly improved over the last decade, and many more such
libraries have been contributed. One such \proglang{C++} library is
\pkg{Eigen} by \citet*{Eigen:Web}. \pkg{Eigen} started as a sub-project to
KDE (a popular Linux desktop environment), initially focussing on fixed-size
matrices which are projections in a visualization application. \pkg{Eigen}
grew from there and has over the course of about a decade produced three
major releases with ``Eigen3'' being the current version.

\pkg{Eigen} is of interest as the \proglang{R} system for statistical
computation and graphics \citep{R:Main} is itself easily extensible. This is
particular true via the \proglang{C} language that most of \proglang{R}'s
compiled core parts are written in, but also for the \proglang{C++} language
which can interface with \proglang{C}-based systems rather easily. The manual
``Writing R Extensions'' \citep{R:Extensions} is the basic reference for
extending \proglang{R} with either \proglang{C} or \proglang{C++}.

The \pkg{Rcpp} package by \citet{JSS:Rcpp,CRAN:Rcpp} facilitates extending
\proglang{R} with \proglang{C++} code by providing seamless object mapping
between both languages.
%
As stated in the \pkg{Rcpp} \citep{CRAN:Rcpp} vignette, ``Extending \pkg{Rcpp}''
\begin{quote}
  \pkg{Rcpp} facilitates data interchange between \proglang{R} and
  \proglang{C++} through the templated functions \texttt{Rcpp::as} (for
  conversion of objects from \proglang{R} to \proglang{C++}) and
  \texttt{Rcpp::wrap} (for conversion from \proglang{C++} to \proglang{R}).
\end{quote}
The \pkg{RcppEigen} package provides the header files composing the
\pkg{Eigen} \proglang{C++} template library and implementations of
\texttt{Rcpp::as} and \texttt{Rcpp::wrap} for the \proglang{C++}
classes defined in \pkg{Eigen}.

The \pkg{Eigen} classes themselves provide high-performance,
versatile and comprehensive representations of dense and sparse
matrices and vectors, as well as decompositions and other functions
to be applied to these objects.  The next section introduces some
of these classes and shows how to interface to them from \proglang{R}.

\section{Eigen classes}
\label{sec:eclasses}

\pkg{Eigen} \citep*{Eigen:Web} is a \proglang{C++} template
library providing classes for many forms of matrices, vectors, arrays
and decompositions.  These classes are flexible and comprehensive
allowing for both high performance and well structured code
representing high-level operations. \proglang{C++} code based on Eigen
is often more like \proglang{R} code, working on the ``whole object'',
rather than compiled code in other languages where operations often must be
coded in loops.

As in many \proglang{C++} template libraries using template meta-programming
\citep{Abrahams+Gurtovoy:2004:TemplateMetaprogramming}, the templates
themselves can be very complicated.  However, \pkg{Eigen} provides
\code{typedef}s for common classes that correspond to \proglang{R} matrices and
vectors, as shown in Table~\ref{tab:REigen}, and this paper will use these
\code{typedef}s throughout this document.
\begin{table}[tb]
  \caption{Correspondence between R matrix and vector types and classes in the \code{Eigen} namespace.}
  \label{tab:REigen}
  \centering
  \begin{tabular}{l l}
    \toprule
    \multicolumn{1}{c}{\proglang{R} object type} & \multicolumn{1}{c}{\pkg{Eigen} class typedef}\\
    \midrule
    numeric matrix                          & \code{MatrixXd}\\
    integer matrix                          & \code{MatrixXi}\\
    complex matrix                          & \code{MatrixXcd}\\
    numeric vector                          & \code{VectorXd}\\
    integer vector                          & \code{VectorXi}\\
    complex vector                          & \code{VectorXcd}\\
    \code{Matrix::dgCMatrix} \phantom{XXX}  & \code{SparseMatrix<double>}\\
    \bottomrule
  \end{tabular}
\end{table}

The \proglang{C++} classes shown in Table~\ref{tab:REigen} are in the
\code{Eigen} namespace, which means that they must be written as
\code{Eigen::MatrixXd}.  However, if one prefaces the use of these class
names with a declaration like

%% Alternatively, use 'highlight --enclose-pre --no-doc --latex --style=emacs --syntax=C++'
%% as the command invoked from C-u M-|
%% For version 3.5 of highlight this should be
%%  highlight --enclose-pre --no-doc --out-format=latex --syntax=C++
%%
%% keep one copy to redo later
%%
%% using Eigen::MatrixXd;
%%
\begin{quote}
\noindent
\ttfamily
\hlstd{}\hlkwa{using\ }\hlstd{Eigen}\hlopt{::}\hlstd{MatrixXd}\hlopt{;}\hlstd{}\hspace*{\fill}\\
\mbox{}
\normalfont
\normalsize
\end{quote}
then one can use these names without the namespace qualifier.

\subsection{Mapped matrices in Eigen}
\label{sec:mapped}

Storage for the contents of matrices from the classes shown in
Table~\ref{tab:REigen} is allocated and controlled by the class
constructors and destructors.  Creating an instance of such a class
from an \proglang{R} object involves copying its contents.  An
alternative is to have the contents of the \proglang{R} matrix or
vector mapped to the contents of the object from the Eigen class.  For
dense matrices one can use the Eigen templated class \code{Map}, and for
sparse matrices one can deploy the Eigen templated class \code{MappedSparseMatrix}.

One must, of course, be careful not to modify the contents of the
\proglang{R} object in the \proglang{C++} code.  A recommended
practice is always to declare mapped objects as {\ttfamily\hlkwb{const}\normalfont}.

\subsection{Arrays in Eigen}
\label{sec:arrays}

For matrix and vector classes \pkg{Eigen} overloads the \texttt{`*'}
operator to indicate matrix multiplication.  Occasionally
component-wise operations instead of matrix operations are preferred.  The
\code{Array} templated classes are used in \pkg{Eigen} for
component-wise operations.  Most often the \code{array()} method is used
for Matrix or Vector objects to create the array.  On those occasions
when one wishes to convert an array to a matrix or vector object
the \code{matrix()} method is used.

\subsection{Structured matrices in \pkg{Eigen}}
\label{sec:structured}

There are \pkg{Eigen} classes for matrices with special structure such
as symmetric matrices, triangular matrices and banded matrices.  For
dense matrices, these special structures are described as ``views'',
meaning that the full dense matrix is stored but only part of the
matrix is used in operations.  For a symmetric matrix one needs to
specify whether the lower triangle or the upper triangle is to be used as
the contents, with the other triangle defined by the implicit symmetry.


\section{Some simple examples}
\label{sec:simple}

\proglang{C++} functions to perform simple operations on matrices or
vectors can follow a pattern of:
\begin{enumerate}
\item Map the \proglang{R} objects passed as arguments into Eigen objects.
\item Create the result.
\item Return \code{Rcpp::wrap} applied to the result.
\end{enumerate}

An idiom for the first step is
%\begin{lstlisting}[language=C++]
% using Eigen::Map;
% using Eigen::MatrixXd;
% using Rcpp::as;

% const Map<MatrixXd>  A(as<Map<MatrixXd> >(AA));
%\end{lstlisting}
\begin{quote}
  \noindent
  \ttfamily
  \hlstd{}\hlkwa{using\ }\hlstd{Eigen}\hlsym{::}\hlstd{Map}\hlsym{;}\hspace*{\fill}\\
  \hlstd{}\hlkwa{using\ }\hlstd{Eigen}\hlsym{::}\hlstd{MatrixXd}\hlsym{;}\hspace*{\fill}\\
  \hlstd{}\hlkwa{using\ }\hlstd{Rcpp}\hlsym{::}\hlstd{as}\hlsym{;}\hspace*{\fill}\\
  \hlstd{}\hspace*{\fill}\\
  \hlkwb{const\ }\hlstd{Map}\hlsym{$<$}\hlstd{MatrixXd}\hlsym{$>$}\hlstd{\ \ }\hlsym{}\hlstd{}\hlkwd{A}\hlstd{}\hlsym{(}\hlstd{as}\hlsym{$<$}\hlstd{Map}\hlsym{$<$}\hlstd{MatrixXd}\hlsym{$>$\ $>$(}\hlstd{AA}\hlsym{));}\hlstd{}\hspace*{\fill}\\
  \mbox{}
  \normalfont
\end{quote}
where \code{AA} is the name of the R object (called an \code{SEXP} in
\proglang{C} and \proglang{C++}) passed to the \proglang{C++} function.

The \Sexpr{link("cxxfunction")} from the \pkg{inline}
package \citep*{CRAN:inline} for \proglang{R} and its \pkg{RcppEigen}
plugin provide a convenient way of developing and debugging the
\proglang{C++} code.  For actual production code one generally
incorporates the \proglang{C++} source code files in a package and
include the line \code{LinkingTo: Rcpp, RcppEigen} in the package's
\code{DESCRIPTION} file.  The
\Sexpr{link("RcppEigen.package.skeleton")} function provides a quick
way of generating the skeleton of a package using \pkg{RcppEigen}
facilities.

The \code{cxxfunction} with the \code{"Rcpp"} or \code{"RcppEigen"}
plugins has the \code{as} and \code{wrap} functions already defined as
\code{Rcpp::as} and \code{Rcpp::wrap}.  In the examples below
these declarations are omitted.  It is important to remember that they are
needed in actual \proglang{C++} source code for a package.

The first few examples are simply for illustration as the operations
shown could be more effectively performed directly in \proglang{R}.
Finally, the results from \pkg{Eigen} are compared to those from the direct
\proglang{R} results.

\subsection{Transpose of an integer matrix}
\label{sec:transpose}

The next \proglang{R} code snippet creates a simple matrix of integers
<<Adef>>=
(A <- matrix(1:6, ncol=2))
str(A)
@
and, in Figure~\ref{trans}, the \code{transpose()} method for the
\code{Eigen::MatrixXi} class is used to return the transpose of the supplied matrix. The \proglang{R}
matrix in the \code{SEXP} \code{AA} is mapped to an
\code{Eigen::MatrixXi} object then the matrix \code{At} is constructed
from its transpose and returned to \proglang{R}.

<<transCpp,echo=FALSE>>=
transCpp <-'
using Eigen::Map;
using Eigen::MatrixXi;
                 // Map the integer matrix AA from R
const Map<MatrixXi>  A(as<Map<MatrixXi> >(AA));
                 // evaluate and return the transpose of A
const MatrixXi      At(A.transpose());
return wrap(At);
'
@
\begin{figure}[htb]
  \begin{quote}
    \noindent
    \ttfamily
    \hlstd{}\hlkwa{using\ }\hlstd{Eigen}\hlsym{::}\hlstd{Map}\hlsym{;}\hspace*{\fill}\\
    \hlstd{}\hlkwa{using\ }\hlstd{Eigen}\hlsym{::}\hlstd{MatrixXi}\hlsym{;}\hspace*{\fill}\\
    \hlstd{}\hlstd{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ }\hlstd{}\hlslc{//\ Map\ the\ integer\ matrix\ AA\ from\ R}\hspace*{\fill}\\
    \hlstd{}\hlkwb{const\ }\hlstd{Map}\hlsym{$<$}\hlstd{MatrixXi}\hlsym{$>$}\hlstd{\ \ }\hlsym{}\hlstd{}\hlkwd{A}\hlstd{}\hlsym{(}\hlstd{as}\hlsym{$<$}\hlstd{Map}\hlsym{$<$}\hlstd{MatrixXi}\hlsym{$>$\ $>$(}\hlstd{AA}\hlsym{));}\hspace*{\fill}\\
    \hlstd{}\hlstd{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ }\hlstd{}\hlslc{//\ evaluate\ and\ return\ the\ transpose\ of\ A}\hspace*{\fill}\\
    \hlstd{}\hlkwb{const\ }\hlstd{MatrixXi}\hlstd{\ \ \ \ \ \ }\hlstd{}\hlkwd{At}\hlstd{}\hlsym{(}\hlstd{A}\hlsym{.}\hlstd{}\hlkwd{transpose}\hlstd{}\hlsym{());}\hspace*{\fill}\\
    \hlstd{}\hlkwa{return\ }\hlstd{}\hlkwd{wrap}\hlstd{}\hlsym{(}\hlstd{At}\hlsym{);}\hlstd{}\hspace*{\fill}
    \normalfont
  \end{quote}
  \caption{\textbf{transCpp}: Transpose a matrix of integers}
  \label{trans}
\end{figure}

The next \proglang{R} snippet compiles and links the \proglang{C++} code
segment (stored as text in a variable named \code{transCpp}) into an
executable function \code{ftrans} and then checks that it works as intended
by compariing the output to an explicit transpose of the matrix argument.
<<ftrans>>=
ftrans <- cxxfunction(signature(AA="matrix"), transCpp, plugin="RcppEigen")
(At <- ftrans(A))
stopifnot(all.equal(At, t(A)))
@

For numeric or integer matrices the \code{adjoint()} method is
equivalent to the \code{transpose()} method.  For complex matrices, the
adjoint is the conjugate of the transpose.  In keeping with the
conventions in the \pkg{Eigen} documentation, in what follows,
the \code{adjoint()} method is used to create the transpose of numeric or
integer matrices.

\subsection{Products and cross-products}
\label{sec:products}

As mentioned in Sec.~\ref{sec:arrays}, the \code{`*'} operator
performs matrix multiplication on \code{Eigen::Matrix} or
\code{Eigen::Vector} objects. The \proglang{C++} code in
Figure~\ref{prod} produces a list containing both the product and
cross-product of its two arguments.

<<prodCpp,echo=FALSE>>=
prodCpp <- '
using Eigen::Map;
using Eigen::MatrixXi;
const Map<MatrixXi>    B(as<Map<MatrixXi> >(BB));
const Map<MatrixXi>    C(as<Map<MatrixXi> >(CC));
return List::create(_["B %*% C"]         = B * C,
                    _["crossprod(B, C)"] = B.adjoint() * C);
'
@
\begin{figure}[htb]
  \begin{quote}
\noindent
\ttfamily
\hlstd{}\hlkwa{using\ }\hlstd{Eigen}\hlopt{::}\hlstd{Map}\hlopt{;}\hspace*{\fill}\\
\hlstd{}\hlkwa{using\ }\hlstd{Eigen}\hlopt{::}\hlstd{MatrixXi}\hlopt{;}\hspace*{\fill}\\
\hlstd{}\hlkwb{const\ }\hlstd{Map}\hlopt{$<$}\hlstd{MatrixXi}\hlopt{$>$}\hlstd{\ \ \ \ }\hlopt{}\hlstd{}\hlkwd{B}\hlstd{}\hlopt{(}\hlstd{as}\hlopt{$<$}\hlstd{Map}\hlopt{$<$}\hlstd{MatrixXi}\hlopt{$>$\ $>$(}\hlstd{BB}\hlopt{));}\hspace*{\fill}\\
\hlstd{}\hlkwb{const\ }\hlstd{Map}\hlopt{$<$}\hlstd{MatrixXi}\hlopt{$>$}\hlstd{\ \ \ \ }\hlopt{}\hlstd{}\hlkwd{C}\hlstd{}\hlopt{(}\hlstd{as}\hlopt{$<$}\hlstd{Map}\hlopt{$<$}\hlstd{MatrixXi}\hlopt{$>$\ $>$(}\hlstd{CC}\hlopt{));}\hspace*{\fill}\\
\hlstd{}\hlkwa{return\ }\hlstd{List}\hlopt{::}\hlstd{}\hlkwd{create}\hlstd{}\hlopt{(}\hlstd{\textunderscore }\hlopt{{[}}\hlstd{}\hlstr{"B\ \%{*}\%\ C"}\hlstd{}\hlopt{{]}}\hlstd{\ \ \ \ \ \ \ \ \ }\hlopt{=\ }\hlstd{B\ }\hlopt{{*}\ }\hlstd{C}\hlopt{,}\hspace*{\fill}\\
\hlstd{}\hlstd{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ }\hlstd{\textunderscore }\hlopt{{[}}\hlstd{}\hlstr{"crossprod(B,\ C)"}\hlstd{}\hlopt{{]}\ =\ }\hlstd{B}\hlopt{.}\hlstd{}\hlkwd{adjoint}\hlstd{}\hlopt{()\ {*}\ }\hlstd{C}\hlopt{);}\hlstd{}\hspace*{\fill}
\normalfont
\normalsize
  \end{quote}
  \caption{\textbf{prodCpp}: Product and cross-product of two matrices}
  \label{prod}
\end{figure}
<<prod>>=
fprod <- cxxfunction(signature(BB = "matrix", CC = "matrix"), prodCpp, "RcppEigen")
B <- matrix(1:4, ncol=2); C <- matrix(6:1, nrow=2)
str(fp <- fprod(B, C))
stopifnot(all.equal(fp[[1]], B %*% C), all.equal(fp[[2]], crossprod(B, C)))
@

Notice that the \code{create} method for the \pkg{Rcpp} class
\code{List} implicitly applies \code{Rcpp::wrap} to its arguments.

\subsection{Crossproduct of a single matrix}
\label{sec:crossproduct}

As shown in the last example, the \proglang{R} function
\code{crossprod} calculates the product of the transpose of its first
argument with its second argument.  The single argument form,
\code{crossprod(X)}, evaluates $\bm X^\prime\bm X$.  One could, of
course, calculate this product as
<<eval=FALSE>>=
t(X) %*% X
@
but \code{crossprod(X)} is roughly twice as fast because the result is
known to be symmetric and only one triangle needs to be calculated.
The function \code{tcrossprod} evaluates \code{crossprod(t(X))}
without actually forming the transpose.

To express these calculations in Eigen, a \code{SelfAdjointView} is created,
which is a dense matrix of which only one triangle is used, the other
triangle being inferred from the symmetry.  (``Self-adjoint'' is
equivalent to symmetric for non-complex matrices.)

The \pkg{Eigen} class name is \code{SelfAdjointView}.  The method for
general matrices that produces such a view is called
\code{selfadjointView}.  Both require specification of either the
\code{Lower} or \code{Upper} triangle.

For triangular matrices the class is \code{TriangularView} and the
method is \code{triangularView}.  The triangle can be specified as
\code{Lower}, \code{UnitLower}, \code{StrictlyLower}, \code{Upper},
\code{UnitUpper} or \code{StrictlyUpper}.

For self-adjoint views the \code{rankUpdate} method adds a scalar multiple
of $\bm A\bm A^\prime$ to the current symmetric matrix.  The scalar
multiple defaults to 1.  The code in Figure~\ref{crossprod} produces

<<crossprod,echo=FALSE>>=
crossprodCpp <- '
using Eigen::Map;
using Eigen::MatrixXi;
using Eigen::Lower;

const Map<MatrixXi> A(as<Map<MatrixXi> >(AA));
const int           m(A.rows()), n(A.cols());
MatrixXi          AtA(MatrixXi(n, n).setZero().
                      selfadjointView<Lower>().rankUpdate(A.adjoint()));
MatrixXi          AAt(MatrixXi(m, m).setZero().
                      selfadjointView<Lower>().rankUpdate(A));

return List::create(_["crossprod(A)"]  = AtA,
                    _["tcrossprod(A)"] = AAt);
'
@
\begin{figure}[htb]
  \begin{quote}
    \noindent
    \ttfamily
    \hlstd{}\hlkwa{using\ }\hlstd{Eigen}\hlopt{::}\hlstd{Map}\hlopt{;}\hspace*{\fill}\\
    \hlstd{}\hlkwa{using\ }\hlstd{Eigen}\hlopt{::}\hlstd{MatrixXi}\hlopt{;}\hspace*{\fill}\\
    \hlstd{}\hlkwa{using\ }\hlstd{Eigen}\hlopt{::}\hlstd{Lower}\hlopt{;}\hspace*{\fill}\\
    \hlstd{}\hspace*{\fill}\\
    \hlkwb{const\ }\hlstd{Map}\hlopt{$<$}\hlstd{MatrixXi}\hlopt{$>$\ }\hlstd{}\hlkwd{A}\hlstd{}\hlopt{(}\hlstd{as}\hlopt{$<$}\hlstd{Map}\hlopt{$<$}\hlstd{MatrixXi}\hlopt{$>$\ $>$(}\hlstd{AA}\hlopt{));}\hspace*{\fill}\\
    \hlstd{}\hlkwb{const\ int}\hlstd{\ \ \ \ \ \ \ \ \ \ \ }\hlkwb{}\hlstd{}\hlkwd{m}\hlstd{}\hlopt{(}\hlstd{A}\hlopt{.}\hlstd{}\hlkwd{rows}\hlstd{}\hlopt{()),\ }\hlstd{}\hlkwd{n}\hlstd{}\hlopt{(}\hlstd{A}\hlopt{.}\hlstd{}\hlkwd{cols}\hlstd{}\hlopt{());}\hspace*{\fill}\\
    \hlstd{MatrixXi}\hlstd{\ \ \ \ \ \ \ \ \ \ }\hlstd{}\hlkwd{AtA}\hlstd{}\hlopt{(}\hlstd{}\hlkwd{MatrixXi}\hlstd{}\hlopt{(}\hlstd{n}\hlopt{,\ }\hlstd{n}\hlopt{).}\hlstd{}\hlkwd{setZero}\hlstd{}\hlopt{().}\hspace*{\fill}\\
    \hlstd{}\hlstd{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ }\hlstd{selfadjointView}\hlopt{$<$}\hlstd{Lower}\hlopt{$>$().}\hlstd{}\hlkwd{rankUpdate}\hlstd{}\hlopt{(}\hlstd{A}\hlopt{.}\hlstd{}\hlkwd{adjoint}\hlstd{}\hlopt{()));}\hspace*{\fill}\\
    \hlstd{MatrixXi}\hlstd{\ \ \ \ \ \ \ \ \ \ }\hlstd{}\hlkwd{AAt}\hlstd{}\hlopt{(}\hlstd{}\hlkwd{MatrixXi}\hlstd{}\hlopt{(}\hlstd{m}\hlopt{,\ }\hlstd{m}\hlopt{).}\hlstd{}\hlkwd{setZero}\hlstd{}\hlopt{().}\hspace*{\fill}\\
    \hlstd{}\hlstd{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ }\hlstd{selfadjointView}\hlopt{$<$}\hlstd{Lower}\hlopt{$>$().}\hlstd{}\hlkwd{rankUpdate}\hlstd{}\hlopt{(}\hlstd{A}\hlopt{));}\hspace*{\fill}\\
    \hlstd{}\hspace*{\fill}\\
    \hlkwa{return\ }\hlstd{List}\hlopt{::}\hlstd{}\hlkwd{create}\hlstd{}\hlopt{(}\hlstd{\textunderscore }\hlopt{{[}}\hlstd{}\hlstr{"crossprod(A)"}\hlstd{}\hlopt{{]}}\hlstd{\ \ }\hlopt{=\ }\hlstd{AtA}\hlopt{,}\hspace*{\fill}\\
    \hlstd{}\hlstd{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ }\hlstd{\textunderscore }\hlopt{{[}}\hlstd{}\hlstr{"tcrossprod(A)"}\hlstd{}\hlopt{{]}\ =\ }\hlstd{AAt}\hlopt{);}\hlstd{}\hspace*{\fill}
    \normalfont
    \normalsize
  \end{quote}
  \caption{\textbf{crossprodCpp}: Cross-product and transposed cross-product of a single matrix}
  \label{crossprod}
\end{figure}
<<>>=
fcprd <- cxxfunction(signature(AA = "matrix"), crossprodCpp, "RcppEigen")
str(crp <- fcprd(A))
stopifnot(all.equal(crp[[1]], crossprod(A)), all.equal(crp[[2]], tcrossprod(A)))
@

To some, the expressions to construct \code{AtA} and \code{AAt} in
that code fragment are compact and elegant.  To others they are
hopelessly confusing.  If you find yourself in the latter group, you
just need to read the expression left to right.  So, for example, we
construct \code{AAt} by creating a general integer matrix of size
$m\times m$ (where $\bm A$ is $m\times n$), ensure that all its
elements are zero, regard it as a self-adjoint (i.e. symmetric) matrix
using the elements in the lower triangle, then add $\bm A\bm A^\prime$
to it and convert back to a general matrix form (i.e. the strict
lower triangle is copied into the strict upper triangle).

For these products one could use either the lower triangle or the upper
triangle as the result will be symmetrized before it is returned.

\subsection{Cholesky decomposition of the crossprod}
\label{sec:chol}

The Cholesky decomposition of the positive-definite, symmetric matrix,
$\bm A$, can be written in several forms.  Numerical analysts define
the ``LLt'' form as the lower triangular matrix, $\bm L$, such that
$\bm A=\bm L\bm L^\prime$ and the ``LDLt'' form as a unit lower
triangular matrix $\bm L$ and a diagonal matrix $\bm D$ with positive
diagonal elements such that $\bm A=\bm L\bm D\bm L^\prime$.
Statisticians often write the decomposition as $\bm A=\bm R^\prime\bm
R$ where $\bm R$ is an upper triangular matrix.  Of course, this $\bm
R$ is simply the transpose of $\bm L$ from the ``LLt'' form.

The templated \pkg{Eigen} classes for the LLt and LDLt forms are
called \code{LLT} and \code{LDLT}.  In general, one would preserve the
objects from these classes in order to re-use them for solutions of
linear systems.  For a simple illustration, the matrix $\bm L$
from the ``LLt'' form is returned.

Because the Cholesky decomposition involves taking square roots, the internal
representation is switched to numeric matrices
<<storage>>=
storage.mode(A) <- "double"
@
before applying the code in Figure~\ref{chol}.
<<cholCpp,echo=FALSE>>=
cholCpp <- '
using Eigen::Map;
using Eigen::MatrixXd;
using Eigen::LLT;
using Eigen::Lower;

const Map<MatrixXd>   A(as<Map<MatrixXd> >(AA));
const int             n(A.cols());
const LLT<MatrixXd> llt(MatrixXd(n, n).setZero().
                        selfadjointView<Lower>().rankUpdate(A.adjoint()));

return List::create(_["L"] = MatrixXd(llt.matrixL()),
                    _["R"] = MatrixXd(llt.matrixU()));
'
@
\begin{figure}[htb]
  \begin{quote}
    \noindent
    \ttfamily
    \hlstd{}\hlkwa{using\ }\hlstd{Eigen}\hlopt{::}\hlstd{Map}\hlopt{;}\hspace*{\fill}\\
    \hlstd{}\hlkwa{using\ }\hlstd{Eigen}\hlopt{::}\hlstd{MatrixXd}\hlopt{;}\hspace*{\fill}\\
    \hlstd{}\hlkwa{using\ }\hlstd{Eigen}\hlopt{::}\hlstd{LLT}\hlopt{;}\hspace*{\fill}\\
    \hlstd{}\hlkwa{using\ }\hlstd{Eigen}\hlopt{::}\hlstd{Lower}\hlopt{;}\hspace*{\fill}\\
    \hlstd{}\hspace*{\fill}\\
    \hlkwb{const\ }\hlstd{Map}\hlopt{$<$}\hlstd{MatrixXd}\hlopt{$>$}\hlstd{\ \ \ }\hlopt{}\hlstd{}\hlkwd{A}\hlstd{}\hlopt{(}\hlstd{as}\hlopt{$<$}\hlstd{Map}\hlopt{$<$}\hlstd{MatrixXd}\hlopt{$>$\ $>$(}\hlstd{AA}\hlopt{));}\hspace*{\fill}\\
    \hlstd{}\hlkwb{const\ int}\hlstd{\ \ \ \ \ \ \ \ \ \ \ \ \ }\hlkwb{}\hlstd{}\hlkwd{n}\hlstd{}\hlopt{(}\hlstd{A}\hlopt{.}\hlstd{}\hlkwd{cols}\hlstd{}\hlopt{());}\hspace*{\fill}\\
    \hlstd{}\hlkwb{const\ }\hlstd{LLT}\hlopt{$<$}\hlstd{MatrixXd}\hlopt{$>$\ }\hlstd{}\hlkwd{llt}\hlstd{}\hlopt{(}\hlstd{}\hlkwd{MatrixXd}\hlstd{}\hlopt{(}\hlstd{n}\hlopt{,\ }\hlstd{n}\hlopt{).}\hlstd{}\hlkwd{setZero}\hlstd{}\hlopt{().}\hspace*{\fill}\\
    \hlstd{}\hlstd{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ }\hlstd{selfadjointView}\hlopt{$<$}\hlstd{Lower}\hlopt{$>$().}\hlstd{}\hlkwd{rankUpdate}\hlstd{}\hlopt{(}\hlstd{A}\hlopt{.}\hlstd{}\hlkwd{adjoint}\hlstd{}\hlopt{()));}\hspace*{\fill}\\
    \hlstd{}\hspace*{\fill}\\
    \hlkwa{return\ }\hlstd{List}\hlopt{::}\hlstd{}\hlkwd{create}\hlstd{}\hlopt{(}\hlstd{\textunderscore }\hlopt{{[}}\hlstd{}\hlstr{"L"}\hlstd{}\hlopt{{]}\ =\ }\hlstd{}\hlkwd{MatrixXd}\hlstd{}\hlopt{(}\hlstd{llt}\hlopt{.}\hlstd{}\hlkwd{matrixL}\hlstd{}\hlopt{()),}\hspace*{\fill}\\
    \hlstd{}\hlstd{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ }\hlstd{\textunderscore }\hlopt{{[}}\hlstd{}\hlstr{"R"}\hlstd{}\hlopt{{]}\ =\ }\hlstd{}\hlkwd{MatrixXd}\hlstd{}\hlopt{(}\hlstd{llt}\hlopt{.}\hlstd{}\hlkwd{matrixU}\hlstd{}\hlopt{()));}\hlstd{}\hspace*{\fill}
    \normalfont
    \normalsize
  \end{quote}
  \caption{\textbf{cholCpp}: Cholesky decomposition of a cross-product}
  \label{chol}
\end{figure}
<<fchol>>=
fchol <- cxxfunction(signature(AA = "matrix"), cholCpp, "RcppEigen")
(ll <- fchol(A))
stopifnot(all.equal(ll[[2]], chol(crossprod(A))))
@

\subsection{Determinant of the cross-product matrix}
\label{sec:determinant}

The ``D-optimal'' criterion for experimental design chooses the design
that maximizes the determinant, $|\bm X^\prime\bm X|$, for the
$n\times p$ model matrix (or Jacobian matrix), $\bm X$.  The
determinant, $|\bm L|$, of the $p\times p$ lower Cholesky factor
$\bm L$, defined so that $\bm L\bm L^\prime=\bm X^\prime\bm X$, is
the product of its diagonal elements, as is the case for any
triangular matrix.  By the properties of determinants,
\begin{displaymath}
  |\bm X^\prime\bm X|=|\bm L\bm L^\prime|=|\bm L|\,|\bm L^\prime|=|\bm L|^2
\end{displaymath}

Alternatively, if using the ``LDLt'' decomposition, $\bm L\bm D\bm
L^\prime=\bm X^\prime\bm X$ where $\bm L$ is unit lower triangular and
$\bm D$ is diagonal then $|\bm X^\prime\bm X|$ is the product of the
diagonal elements of $\bm D$.  Because it is known that the diagonals of
$\bm D$ must be non-negative, one often evaluates the logarithm of the
determinant as the sum of the logarithms of the diagonal elements of
$\bm D$.  Several options are shown in Figure~\ref{cholDet}.

<<echo=FALSE>>=
cholDetCpp <- '
using Eigen::Lower;
using Eigen::Map;
using Eigen::MatrixXd;
using Eigen::VectorXd;

const Map<MatrixXd>   A(as<Map<MatrixXd> >(AA));
const int             n(A.cols());
const MatrixXd      AtA(MatrixXd(n, n).setZero().
                        selfadjointView<Lower>().rankUpdate(A.adjoint()));
const MatrixXd     Lmat(AtA.llt().matrixL());
const double       detL(Lmat.diagonal().prod());
const VectorXd     Dvec(AtA.ldlt().vectorD());

return List::create(_["d1"] = detL * detL,
                    _["d2"] = Dvec.prod(),
                    _["ld"] = Dvec.array().log().sum());
'
@
\begin{figure}[htb]
  \begin{quote}
    \noindent
    \ttfamily
    \hlstd{}\hlkwa{using\ }\hlstd{Eigen}\hlopt{::}\hlstd{Lower}\hlopt{;}\hspace*{\fill}\\
    \hlstd{}\hlkwa{using\ }\hlstd{Eigen}\hlopt{::}\hlstd{Map}\hlopt{;}\hspace*{\fill}\\
    \hlstd{}\hlkwa{using\ }\hlstd{Eigen}\hlopt{::}\hlstd{MatrixXd}\hlopt{;}\hspace*{\fill}\\
    \hlstd{}\hlkwa{using\ }\hlstd{Eigen}\hlopt{::}\hlstd{VectorXd}\hlopt{;}\hspace*{\fill}\\
    \hlstd{}\hspace*{\fill}\\
    \hlkwb{const\ }\hlstd{Map}\hlopt{$<$}\hlstd{MatrixXd}\hlopt{$>$}\hlstd{\ \ \ }\hlopt{}\hlstd{}\hlkwd{A}\hlstd{}\hlopt{(}\hlstd{as}\hlopt{$<$}\hlstd{Map}\hlopt{$<$}\hlstd{MatrixXd}\hlopt{$>$\ $>$(}\hlstd{AA}\hlopt{));}\hspace*{\fill}\\
    \hlstd{}\hlkwb{const\ int}\hlstd{\ \ \ \ \ \ \ \ \ \ \ \ \ }\hlkwb{}\hlstd{}\hlkwd{n}\hlstd{}\hlopt{(}\hlstd{A}\hlopt{.}\hlstd{}\hlkwd{cols}\hlstd{}\hlopt{());}\hspace*{\fill}\\
    \hlstd{}\hlkwb{const\ }\hlstd{MatrixXd}\hlstd{\ \ \ \ \ \ }\hlstd{}\hlkwd{AtA}\hlstd{}\hlopt{(}\hlstd{}\hlkwd{MatrixXd}\hlstd{}\hlopt{(}\hlstd{n}\hlopt{,\ }\hlstd{n}\hlopt{).}\hlstd{}\hlkwd{setZero}\hlstd{}\hlopt{().}\hspace*{\fill}\\
    \hlstd{}\hlstd{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ }\hlstd{selfadjointView}\hlopt{$<$}\hlstd{Lower}\hlopt{$>$().}\hlstd{}\hlkwd{rankUpdate}\hlstd{}\hlopt{(}\hlstd{A}\hlopt{.}\hlstd{}\hlkwd{adjoint}\hlstd{}\hlopt{()));}\hspace*{\fill}\\
    \hlstd{}\hlkwb{const\ }\hlstd{MatrixXd}\hlstd{\ \ \ \ \ }\hlstd{}\hlkwd{Lmat}\hlstd{}\hlopt{(}\hlstd{AtA}\hlopt{.}\hlstd{}\hlkwd{llt}\hlstd{}\hlopt{().}\hlstd{}\hlkwd{matrixL}\hlstd{}\hlopt{());}\hspace*{\fill}\\
    \hlstd{}\hlkwb{const\ double}\hlstd{\ \ \ \ \ \ \ }\hlkwb{}\hlstd{}\hlkwd{detL}\hlstd{}\hlopt{(}\hlstd{Lmat}\hlopt{.}\hlstd{}\hlkwd{diagonal}\hlstd{}\hlopt{().}\hlstd{}\hlkwd{prod}\hlstd{}\hlopt{());}\hspace*{\fill}\\
    \hlstd{}\hlkwb{const\ }\hlstd{VectorXd}\hlstd{\ \ \ \ \ }\hlstd{}\hlkwd{Dvec}\hlstd{}\hlopt{(}\hlstd{AtA}\hlopt{.}\hlstd{}\hlkwd{ldlt}\hlstd{}\hlopt{().}\hlstd{}\hlkwd{vectorD}\hlstd{}\hlopt{());}\hspace*{\fill}\\
    \hlstd{}\hspace*{\fill}\\
    \hlkwa{return\ }\hlstd{List}\hlopt{::}\hlstd{}\hlkwd{create}\hlstd{}\hlopt{(}\hlstd{\textunderscore }\hlopt{{[}}\hlstd{}\hlstr{"d1"}\hlstd{}\hlopt{{]}\ =\ }\hlstd{detL\ }\hlopt{{*}\ }\hlstd{detL}\hlopt{,}\hspace*{\fill}\\
    \hlstd{}\hlstd{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ }\hlstd{\textunderscore }\hlopt{{[}}\hlstd{}\hlstr{"d2"}\hlstd{}\hlopt{{]}\ =\ }\hlstd{Dvec}\hlopt{.}\hlstd{}\hlkwd{prod}\hlstd{}\hlopt{(),}\hspace*{\fill}\\
    \hlstd{}\hlstd{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ }\hlstd{\textunderscore }\hlopt{{[}}\hlstd{}\hlstr{"ld"}\hlstd{}\hlopt{{]}\ =\ }\hlstd{Dvec}\hlopt{.}\hlstd{}\hlkwd{array}\hlstd{}\hlopt{().}\hlstd{}\hlkwd{log}\hlstd{}\hlopt{().}\hlstd{}\hlkwd{sum}\hlstd{}\hlopt{());}\hlstd{}\hspace*{\fill}
    \normalfont
    \normalsize
  \end{quote}
  \caption{\textbf{cholDetCpp}: Determinant of a cross-product using the Cholesky decomposition}
  \label{cholDet}
\end{figure}
<<fdet>>=
fdet <- cxxfunction(signature(AA = "matrix"), cholDetCpp, "RcppEigen")
unlist(ll <- fdet(A))
@

Note the use of the \code{array()} method in the calculation of the
log-determinant.  Because the \code{log()} method applies to arrays,
not to vectors or matrices, an array from \code{Dvec} has to be created
before applying the \code{log()} method.

\section{Least squares solutions}
\label{sec:leastSquares}

A common operation in statistical computing is calculating a least
squares solution, $\widehat{\bm\beta}$, defined as
\begin{displaymath}
  \widehat{\bm\beta}=\arg\min_{\beta}\|\bm y-\bm X\bm\beta\|^2
\end{displaymath}
where the model matrix, $\bm X$, is $n\times p$ ($n\ge p$) and $\bm y$
is an $n$-dimensional response vector.  There are several ways, based
on matrix decompositions, to determine such a solution.  Earlier, two forms
of the Cholesky decomposition were discussed: ``LLt'' and
``LDLt'', which can both be used to solve for $\widehat{\bm\beta}$.  Other
decompositions that can be used are the QR decomposition, with or
without column pivoting, the singular value decomposition and the
eigendecomposition of a symmetric matrix.

Determining a least squares solution is relatively straightforward.
However, statistical computing often requires additional information,
such as the standard errors of the coefficient estimates.  Calculating
these involves evaluating the diagonal elements of $\left(\bm
  X^\prime\bm X\right)^{-1}$ and the residual sum of squares, $\|\bm
y-\bm X\widehat{\bm\beta}\|^2$.

\subsection{Least squares using the ``LLt'' Cholesky}
\label{sec:LLtLeastSquares}

<<lltLSCpp,echo=FALSE>>=
lltLSCpp <- '
using Eigen::LLT;
using Eigen::Lower;
using Eigen::Map;
using Eigen::MatrixXd;
using Eigen::VectorXd;

const Map<MatrixXd>   X(as<Map<MatrixXd> >(XX));
const Map<VectorXd>   y(as<Map<VectorXd> >(yy));
const int             n(X.rows()), p(X.cols());
const LLT<MatrixXd> llt(MatrixXd(p, p).setZero().
                        selfadjointView<Lower>().rankUpdate(X.adjoint()));
const VectorXd  betahat(llt.solve(X.adjoint() * y));
const VectorXd   fitted(X * betahat);
const VectorXd    resid(y - fitted);
const int            df(n - p);
const double          s(resid.norm() / std::sqrt(double(df)));
const VectorXd       se(s * llt.matrixL().solve(MatrixXd::Identity(p, p)).
                        colwise().norm());
return     List::create(_["coefficients"]   = betahat,
                        _["fitted.values"]  = fitted,
                        _["residuals"]      = resid,
                        _["s"]              = s,
                        _["df.residual"]    = df,
                        _["rank"]           = p,
                        _["Std. Error"]     = se);
'
@
\begin{figure}[tbh]
  \begin{quote}
    \noindent
    \ttfamily
    \hlstd{}\hlkwa{using\ }\hlstd{Eigen}\hlopt{::}\hlstd{LLT}\hlopt{;}\hspace*{\fill}\\
    \hlstd{}\hlkwa{using\ }\hlstd{Eigen}\hlopt{::}\hlstd{Lower}\hlopt{;}\hspace*{\fill}\\
    \hlstd{}\hlkwa{using\ }\hlstd{Eigen}\hlopt{::}\hlstd{Map}\hlopt{;}\hspace*{\fill}\\
    \hlstd{}\hlkwa{using\ }\hlstd{Eigen}\hlopt{::}\hlstd{MatrixXd}\hlopt{;}\hspace*{\fill}\\
    \hlstd{}\hlkwa{using\ }\hlstd{Eigen}\hlopt{::}\hlstd{VectorXd}\hlopt{;}\hspace*{\fill}\\
    \hlstd{}\hspace*{\fill}\\
    \hlkwb{const\ }\hlstd{Map}\hlopt{$<$}\hlstd{MatrixXd}\hlopt{$>$}\hlstd{\ \ \ }\hlopt{}\hlstd{}\hlkwd{X}\hlstd{}\hlopt{(}\hlstd{as}\hlopt{$<$}\hlstd{Map}\hlopt{$<$}\hlstd{MatrixXd}\hlopt{$>$\ $>$(}\hlstd{XX}\hlopt{));}\hspace*{\fill}\\
    \hlstd{}\hlkwb{const\ }\hlstd{Map}\hlopt{$<$}\hlstd{VectorXd}\hlopt{$>$}\hlstd{\ \ \ }\hlopt{}\hlstd{}\hlkwd{y}\hlstd{}\hlopt{(}\hlstd{as}\hlopt{$<$}\hlstd{Map}\hlopt{$<$}\hlstd{VectorXd}\hlopt{$>$\ $>$(}\hlstd{yy}\hlopt{));}\hspace*{\fill}\\
    \hlstd{}\hlkwb{const\ int}\hlstd{\ \ \ \ \ \ \ \ \ \ \ \ \ }\hlkwb{}\hlstd{}\hlkwd{n}\hlstd{}\hlopt{(}\hlstd{X}\hlopt{.}\hlstd{}\hlkwd{rows}\hlstd{}\hlopt{()),\ }\hlstd{}\hlkwd{p}\hlstd{}\hlopt{(}\hlstd{X}\hlopt{.}\hlstd{}\hlkwd{cols}\hlstd{}\hlopt{());}\hspace*{\fill}\\
    \hlstd{}\hlkwb{const\ }\hlstd{LLT}\hlopt{$<$}\hlstd{MatrixXd}\hlopt{$>$\ }\hlstd{}\hlkwd{llt}\hlstd{}\hlopt{(}\hlstd{}\hlkwd{MatrixXd}\hlstd{}\hlopt{(}\hlstd{p}\hlopt{,\ }\hlstd{p}\hlopt{).}\hlstd{}\hlkwd{setZero}\hlstd{}\hlopt{().}\hspace*{\fill}\\
    \hlstd{}\hlstd{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ }\hlstd{selfadjointView}\hlopt{$<$}\hlstd{Lower}\hlopt{$>$().}\hlstd{}\hlkwd{rankUpdate}\hlstd{}\hlopt{(}\hlstd{X}\hlopt{.}\hlstd{}\hlkwd{adjoint}\hlstd{}\hlopt{()));}\hspace*{\fill}\\
    \hlstd{}\hlkwb{const\ }\hlstd{VectorXd}\hlstd{\ \ }\hlstd{}\hlkwd{betahat}\hlstd{}\hlopt{(}\hlstd{llt}\hlopt{.}\hlstd{}\hlkwd{solve}\hlstd{}\hlopt{(}\hlstd{X}\hlopt{.}\hlstd{}\hlkwd{adjoint}\hlstd{}\hlopt{()\ {*}\ }\hlstd{y}\hlopt{));}\hspace*{\fill}\\
    \hlstd{}\hlkwb{const\ }\hlstd{VectorXd}\hlstd{\ \ \ }\hlstd{}\hlkwd{fitted}\hlstd{}\hlopt{(}\hlstd{X\ }\hlopt{{*}\ }\hlstd{betahat}\hlopt{);}\hspace*{\fill}\\
    \hlstd{}\hlkwb{const\ }\hlstd{VectorXd}\hlstd{\ \ \ \ }\hlstd{}\hlkwd{resid}\hlstd{}\hlopt{(}\hlstd{y\ }\hlopt{{-}\ }\hlstd{fitted}\hlopt{);}\hspace*{\fill}\\
    \hlstd{}\hlkwb{const\ int}\hlstd{\ \ \ \ \ \ \ \ \ \ \ \ }\hlkwb{}\hlstd{}\hlkwd{df}\hlstd{}\hlopt{(}\hlstd{n\ }\hlopt{{-}\ }\hlstd{p}\hlopt{);}\hspace*{\fill}\\
    \hlstd{}\hlkwb{const\ double}\hlstd{\ \ \ \ \ \ \ \ \ \ }\hlkwb{}\hlstd{}\hlkwd{s}\hlstd{}\hlopt{(}\hlstd{resid}\hlopt{.}\hlstd{}\hlkwd{norm}\hlstd{}\hlopt{()\ /\ }\hlstd{std}\hlopt{::}\hlstd{}\hlkwd{sqrt}\hlstd{}\hlopt{(}\hlstd{}\hlkwb{double}\hlstd{}\hlopt{(}\hlstd{df}\hlopt{)));}\hspace*{\fill}\\
    \hlstd{}\hlkwb{const\ }\hlstd{VectorXd}\hlstd{\ \ \ \ \ \ \ }\hlstd{}\hlkwd{se}\hlstd{}\hlopt{(}\hlstd{s\ }\hlopt{{*}\ }\hlstd{llt}\hlopt{.}\hlstd{}\hlkwd{matrixL}\hlstd{}\hlopt{().}\hlstd{}\hlkwd{solve}\hlstd{}\hlopt{(}\hlstd{MatrixXd}\hlopt{::}\hlstd{}\hlkwd{Identity}\hlstd{}\hlopt{(}\hlstd{p}\hlopt{,\ }\hlstd{p}\hlopt{)).}\hspace*{\fill}\\
    \hlstd{}\hlstd{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ }\hlstd{}\hlkwd{colwise}\hlstd{}\hlopt{().}\hlstd{}\hlkwd{norm}\hlstd{}\hlopt{());}\hspace*{\fill}\\
    \hlstd{}\hlkwa{return}\hlstd{\ \ \ \ \ }\hlkwa{}\hlstd{List}\hlopt{::}\hlstd{}\hlkwd{create}\hlstd{}\hlopt{(}\hlstd{\textunderscore }\hlopt{{[}}\hlstd{}\hlstr{"coefficients"}\hlstd{}\hlopt{{]}}\hlstd{\ \ \ }\hlopt{=\ }\hlstd{betahat}\hlopt{,}\hspace*{\fill}\\
    \hlstd{}\hlstd{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ }\hlstd{\textunderscore }\hlopt{{[}}\hlstd{}\hlstr{"fitted.values"}\hlstd{}\hlopt{{]}}\hlstd{\ \ }\hlopt{=\ }\hlstd{fitted}\hlopt{,}\hspace*{\fill}\\
    \hlstd{}\hlstd{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ }\hlstd{\textunderscore }\hlopt{{[}}\hlstd{}\hlstr{"residuals"}\hlstd{}\hlopt{{]}}\hlstd{\ \ \ \ \ \ }\hlopt{=\ }\hlstd{resid}\hlopt{,}\hspace*{\fill}\\
    \hlstd{}\hlstd{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ }\hlstd{\textunderscore }\hlopt{{[}}\hlstd{}\hlstr{"s"}\hlstd{}\hlopt{{]}}\hlstd{\ \ \ \ \ \ \ \ \ \ \ \ \ \ }\hlopt{=\ }\hlstd{s}\hlopt{,}\hspace*{\fill}\\
    \hlstd{}\hlstd{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ }\hlstd{\textunderscore }\hlopt{{[}}\hlstd{}\hlstr{"df.residual"}\hlstd{}\hlopt{{]}}\hlstd{\ \ \ \ }\hlopt{=\ }\hlstd{df}\hlopt{,}\hspace*{\fill}\\
    \hlstd{}\hlstd{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ }\hlstd{\textunderscore }\hlopt{{[}}\hlstd{}\hlstr{"rank"}\hlstd{}\hlopt{{]}}\hlstd{\ \ \ \ \ \ \ \ \ \ \ }\hlopt{=\ }\hlstd{p}\hlopt{,}\hspace*{\fill}\\
    \hlstd{}\hlstd{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ }\hlstd{\textunderscore }\hlopt{{[}}\hlstd{}\hlstr{"Std.\ Error"}\hlstd{}\hlopt{{]}}\hlstd{\ \ \ \ \ }\hlopt{=\ }\hlstd{se}\hlopt{);}\hlstd{}\hspace*{\fill}
    \normalfont
    \normalsize
  \end{quote}
  \caption{\textbf{lltLSCpp}: Least squares using the Cholesky decomposition}
  \label{lltLS}
\end{figure}
Figure~\ref{lltLS} shows a calculation of the least squares coefficient estimates
(\code{betahat}) and the standard errors (\code{se}) through an
``LLt'' Cholesky decomposition of the crossproduct of the model
matrix, $\bm X$.  Next, the results from this calculation are compared
to those from the \code{lm.fit} function in \proglang{R}
(\code{lm.fit} is the workhorse function called by \code{lm} once the
model matrix and response have been evaluated).
<<lltLS>>=
lltLS <- cxxfunction(signature(XX = "matrix", yy = "numeric"), lltLSCpp, "RcppEigen")
data(trees, package="datasets")
str(lltFit <- with(trees, lltLS(cbind(1, log(Girth)), log(Volume))))
str(lmFit <- with(trees, lm.fit(cbind(1, log(Girth)), log(Volume))))
for (nm in c("coefficients", "residuals", "fitted.values", "rank"))
    stopifnot(all.equal(lltFit[[nm]], unname(lmFit[[nm]])))
stopifnot(all.equal(lltFit[["Std. Error"]],
                    unname(coef(summary(lm(log(Volume) ~ log(Girth), trees)))[,2])))
@

There are several aspects of the \proglang{C++} code in
Figure~\ref{lltLS} worth mentioning.  The \code{solve} method for the
\code{LLT} object evaluates, in this case, $\left(\bm X^\prime\bm
  X\right)^{-1}\bm X^\prime\bm y$ but without actually evaluating the
inverse.  The calculation of the residuals, $\bm y-\widehat{\bm y}$,
can be written, as in \proglang{R}, as \code{y - fitted}. (But note
that \pkg{Eigen} classes do not have a ``recycling rule as in
\proglang{R}.  That is, the two vector operands must have the same
length.)  The \code{norm()} method evaluates the square root of the
sum of squares of the elements of a vector.  Although one does not
explicitly evaluate $\left(\bm X^\prime\bm X\right)^{-1}$ one does
evaluate $\bm L^{-1}$ to obtain the standard errors.  Note also the
use of the \code{colwise()} method in the evaluation of the standard
errors.  It applies a method to the columns of a matrix, returning a
vector.  The \pkg{Eigen} \code{colwise()} and \code{rowwise()} methods
are similar in effect to the \code{apply} function in \proglang{R}.

In the descriptions of other methods for solving least squares
problems, much of the code parallels that shown in
Figure~\ref{lltLS}.  The redundant parts are omitted, and only
the evaluation of the coefficients, the rank and the standard errors is shown.
Actually, the standard errors are calculated only up to the scalar
multiple of $s$, the residual standard error, in these code fragments.
The calculation of the residuals and $s$ and the scaling of the
coefficient standard errors is the same for all methods.  (See the
files \code{fastLm.h} and \code{fastLm.cpp} in the \pkg{RcppEigen}
source package for details.)

\subsection{Least squares using the unpivoted QR decomposition}
\label{sec:QR}

A QR decomposition has the form
\begin{displaymath}
  \bm X=\bm Q\bm R=\bm Q_1\bm R_1
\end{displaymath}
where $\bm Q$ is an $n\times n$ orthogonal matrix, which means that
$\bm Q^\prime\bm Q=\bm Q\bm Q^\prime=\bm I_n$, and the $n\times p$
matrix $\bm R$ is zero below the main diagonal.  The $n\times p$
matrix $\bm Q_1$ is the first $p$ columns of $\bm Q$ and the $p\times
p$ upper triangular matrix $\bm R_1$ is the top $p$ rows of $\bm R$.
There are three \pkg{Eigen} classes for the QR decomposition:
\code{HouseholderQR} provides the basic QR decomposition using
Householder transformations, \code{ColPivHouseholderQR} incorporates
column pivots and \code{FullPivHouseholderQR} incorporates both row
and column pivots.

Figure~\ref{QRLS} shows a least squares solution using the unpivoted
QR decomposition.
% using Eigen::HouseholderQR;

% const HouseholderQR<MatrixXd> QR(X);
% const VectorXd           betahat(QR.solve(y));
% const VectorXd            fitted(X * betahat);
% const int                     df(n - p);
% const VectorXd                se(QR.matrixQR().topRows(p).
%                                  triangularView<Upper>().
%                                  solve(MatrixXd::Identity(p,p)).
%                                  rowwise().norm());
\begin{figure}[htb]
  \begin{quote}
    \noindent
    \ttfamily
    \hlstd{}\hlkwa{using\ }\hlstd{Eigen}\hlopt{::}\hlstd{HouseholderQR}\hlopt{;}\hspace*{\fill}\\
    \hlstd{}\hspace*{\fill}\\
    \hlkwb{const\ }\hlstd{HouseholderQR}\hlopt{$<$}\hlstd{MatrixXd}\hlopt{$>$\ }\hlstd{}\hlkwd{QR}\hlstd{}\hlopt{(}\hlstd{X}\hlopt{);}\hspace*{\fill}\\
    \hlstd{}\hlkwb{const\ }\hlstd{VectorXd}\hlstd{\ \ \ \ \ \ \ \ \ \ \ }\hlstd{}\hlkwd{betahat}\hlstd{}\hlopt{(}\hlstd{QR}\hlopt{.}\hlstd{}\hlkwd{solve}\hlstd{}\hlopt{(}\hlstd{y}\hlopt{));}\hspace*{\fill}\\
    \hlstd{}\hlkwb{const\ }\hlstd{VectorXd}\hlstd{\ \ \ \ \ \ \ \ \ \ \ \ }\hlstd{}\hlkwd{fitted}\hlstd{}\hlopt{(}\hlstd{X\ }\hlopt{{*}\ }\hlstd{betahat}\hlopt{);}\hspace*{\fill}\\
    \hlstd{}\hlkwb{const\ int}\hlstd{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ }\hlkwb{}\hlstd{}\hlkwd{df}\hlstd{}\hlopt{(}\hlstd{n\ }\hlopt{{-}\ }\hlstd{p}\hlopt{);}\hspace*{\fill}\\
    \hlstd{}\hlkwb{const\ }\hlstd{VectorXd}\hlstd{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ }\hlstd{}\hlkwd{se}\hlstd{}\hlopt{(}\hlstd{QR}\hlopt{.}\hlstd{}\hlkwd{matrixQR}\hlstd{}\hlopt{().}\hlstd{}\hlkwd{topRows}\hlstd{}\hlopt{(}\hlstd{p}\hlopt{).}\hspace*{\fill}\\
    \hlstd{}\hlstd{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ }\hlstd{triangularView}\hlopt{$<$}\hlstd{Upper}\hlopt{$>$().}\hspace*{\fill}\\
    \hlstd{}\hlstd{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ }\hlstd{}\hlkwd{solve}\hlstd{}\hlopt{(}\hlstd{MatrixXd}\hlopt{::}\hlstd{}\hlkwd{Identity}\hlstd{}\hlopt{(}\hlstd{p}\hlopt{,}\hlstd{p}\hlopt{)).}\hspace*{\fill}\\
    \hlstd{}\hlstd{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ }\hlstd{}\hlkwd{rowwise}\hlstd{}\hlopt{().}\hlstd{}\hlkwd{norm}\hlstd{}\hlopt{());}\hlstd{}\hspace*{\fill}
    \normalfont
    \normalsize
    \caption{\textbf{QRLSCpp}: Least squares using the unpivoted QR decomposition}
    \label{QRLS}
  \end{quote}
\end{figure}
The calculations in Figure~\ref{QRLS} are quite similar to those in
Figure~\ref{lltLS}.  In fact, if one had extracted the upper
triangular factor (the \code{matrixU()} method) from the \code{LLT}
object in Figure~\ref{lltLS}, the rest of the code would be nearly
identical.

\subsection{Handling the rank-deficient case}
\label{sec:rankdeficient}

One important consideration when determining least squares solutions
is whether $\rank(\bm X)$ is $p$, a situation described by saying
that $\bm X$ has ``full column rank''.   When $\bm X$ does not have
full column rank it is said to be ``rank deficient''.

Although the theoretical rank of a matrix is well-defined, its
evaluation in practice is not.  At best one can compute an effective
rank according to some tolerance.  Decompositions that allow to
estimation of the rank of the matrix in this way are said to be
``rank-revealing''.

Because the \code{model.matrix} function in \proglang{R} does a
considerable amount of symbolic analysis behind the scenes, one usually
ends up with full-rank model matrices.  The common cases of
rank-deficiency, such as incorporating both a constant term and a full
set of indicators columns for the levels of a factor, are eliminated.
Other, more subtle, situations will not be detected at this stage,
however.  A simple example occurs when there is a ``missing cell'' in a
two-way layout and the interaction of the two factors is included in
the model.

<<missingcell>>=
dd <- data.frame(f1 = gl(4, 6, labels = LETTERS[1:4]),
                 f2 = gl(3, 2, labels = letters[1:3]))[-(7:8), ]
xtabs(~ f2 + f1, dd)                    # one missing cell
mm <- model.matrix(~ f1 * f2, dd)
kappa(mm)         # large condition number, indicating rank deficiency
rcond(mm)         # alternative evaluation, the reciprocal condition number
(c(rank=qr(mm)$rank, p=ncol(mm))) # rank as computed in R's qr function
set.seed(1)
dd$y <- mm %*% seq_len(ncol(mm)) + rnorm(nrow(mm), sd = 0.1)
                         # lm detects the rank deficiency
fm1 <- lm(y ~ f1 * f2, dd)
writeLines(capture.output(print(summary(fm1), signif.stars=FALSE))[9:22])
@

The \code{lm} function for fitting linear models in \proglang{R} uses
a rank-revealing form of the QR decomposition.  When the model matrix
is determined to be rank deficient, according to the threshold used in
\proglang{R}'s QR decomposition, the model matrix is reduced to
$\rank{(\bm X)}$ columns by pivoting selected columns (those that are
apparently linearly dependent on columns to their left) to the right
hand side of the matrix.  A solution for this reduced model matrix is
determined and the coefficients and standard errors for the redundant
columns are flagged as missing.

An alternative approach is to evaluate the ``pseudo-inverse'' of $\bm
X$ from the singular value decomposition (SVD) of $\bm X$ or the
eigendecomposition of $\bm X^\prime\bm X$.  The SVD is of the form
\begin{displaymath}
  \bm X=\bm U\bm D\bm V^\prime=\bm U_1\bm D_1\bm V^\prime
\end{displaymath}
where $\bm U$ is an orthogonal $n\times n$ matrix and $\bm U_1$ is its
leftmost $p$ columns, $\bm D$ is $n\times p$ and zero off the main
diagonal so that $\bm D_1$ is a $p\times p$ diagonal matrix with
non-decreasing non-negative diagonal elements, and $\bm V$ is a $p\times
p$ orthogonal matrix.  The pseudo-inverse of $\bm D_1$, written $\bm
D_1^+$ is a $p\times p$ diagonal matrix whose first $r=\rank(\bm X)$
diagonal elements are the inverses of the corresponding diagonal
elements of $\bm D_1$ and whose last $p-r$ diagonal elements are zero.

The tolerance for determining if an element of the diagonal of $\bm D$
is considered to be (effectively) zero is a multiple of the largest
singular value (i.e. the $(1,1)$ element of $\bm D$).

In Figure~\ref{Dplus} a utility function, \code{Dplus}, is defined to
return the pseudo-inverse as a diagonal matrix, given the singular
values (the diagonal of $\bm D$) and the apparent rank.  To be able to
use this function with the eigendecomposition where the eigenvalues
are in increasing order, a Boolean argument \code{rev} is included
indicating whether the order is reversed.

\begin{figure}[htb]
  \begin{quote}
    \noindent
    \ttfamily
    \hlstd{}\hlkwa{using\ }\hlstd{Eigen}\hlopt{::}\hlstd{DiagonalMatrix}\hlopt{;}\hspace*{\fill}\\
    \hlstd{}\hlkwa{using\ }\hlstd{Eigen}\hlopt{::}\hlstd{Dynamic}\hlopt{;}\hspace*{\fill}\\
    \hlstd{}\hspace*{\fill}\\
    \hlkwc{inline\ }\hlstd{DiagonalMatrix}\hlopt{$<$}\hlstd{}\hlkwb{double}\hlstd{}\hlopt{,\ }\hlstd{Dynamic}\hlopt{$>$\ }\hlstd{}\hlkwd{Dplus}\hlstd{}\hlopt{(}\hlstd{}\hlkwb{const\ }\hlstd{ArrayXd}\hlopt{\&\ }\hlstd{D}\hlopt{,}\hspace*{\fill}\\
    \hlstd{}\hlstd{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ }\hlstd{}\hlkwb{int\ }\hlstd{r}\hlopt{,\ }\hlstd{}\hlkwb{bool\ }\hlstd{rev}\hlopt{=}\hlstd{}\hlkwa{false}\hlstd{}\hlopt{)\ \{}\hspace*{\fill}\\
    \hlstd{}\hlstd{\ \ \ \ }\hlstd{VectorXd}\hlstd{\ \ \ }\hlstd{}\hlkwd{Di}\hlstd{}\hlopt{(}\hlstd{VectorXd}\hlopt{::}\hlstd{}\hlkwd{Constant}\hlstd{}\hlopt{(}\hlstd{D}\hlopt{.}\hlstd{}\hlkwd{size}\hlstd{}\hlopt{(),\ }\hlstd{}\hlnum{0}\hlstd{}\hlopt{.));}\hspace*{\fill}\\
    \hlstd{}\hlstd{\ \ \ \ }\hlstd{}\hlkwa{if\ }\hlstd{}\hlopt{(}\hlstd{rev}\hlopt{)\ }\hlstd{Di}\hlopt{.}\hlstd{}\hlkwd{tail}\hlstd{}\hlopt{(}\hlstd{r}\hlopt{)}\hlstd{\ \ }\hlopt{=\ }\hlstd{D}\hlopt{.}\hlstd{}\hlkwd{tail}\hlstd{}\hlopt{(}\hlstd{r}\hlopt{).}\hlstd{}\hlkwd{inverse}\hlstd{}\hlopt{();}\hspace*{\fill}\\
    \hlstd{}\hlstd{\ \ \ \ }\hlstd{}\hlkwa{else\ }\hlstd{Di}\hlopt{.}\hlstd{}\hlkwd{head}\hlstd{}\hlopt{(}\hlstd{r}\hlopt{)}\hlstd{\ \ \ \ \ \ }\hlopt{=\ }\hlstd{D}\hlopt{.}\hlstd{}\hlkwd{head}\hlstd{}\hlopt{(}\hlstd{r}\hlopt{).}\hlstd{}\hlkwd{inverse}\hlstd{}\hlopt{();}\hspace*{\fill}\\
    \hlstd{}\hlstd{\ \ \ \ }\hlstd{}\hlkwa{return\ }\hlstd{DiagonalMatrix}\hlopt{$<$}\hlstd{}\hlkwb{double}\hlstd{}\hlopt{,\ }\hlstd{Dynamic}\hlopt{$>$(}\hlstd{Di}\hlopt{);}\hspace*{\fill}\\
    \hlstd{}\hlopt{\}}\hlstd{}\hspace*{\fill}
    \normalfont
    \normalsize
  \end{quote}
  \caption{\textbf{DplusCpp}: Create the diagonal matrix $\bm D^+$ from the array of singular values $\bm d$}
  \label{Dplus}
\end{figure}
% using Eigen::DiagonalMatrix;
% using Eigen::Dynamic;

% inline DiagonalMatrix<double, Dynamic> Dplus(const ArrayXd& D,
%                                              int r, bool rev=false) {
%     VectorXd   Di(VectorXd::Constant(D.size(), 0.));
%     if (rev) Di.tail(r)  = D.tail(r).inverse();
%     else Di.head(r)      = D.head(r).inverse();
%     return DiagonalMatrix<double, Dynamic>(Di);
% }

\subsection{Least squares using the SVD}
\label{sec:SVDls}

With these definitions the code for least squares using the singular
value decomposition can be written as in Figure~\ref{SVDLS}.
\begin{figure}[htb]
  \begin{quote}
    \noindent
    \ttfamily
    \hlstd{}\hlkwa{using\ }\hlstd{Eigen}\hlopt{::}\hlstd{JacobiSVD}\hlopt{;}\hspace*{\fill}\\
    \hlstd{}\hspace*{\fill}\\
    \hlkwb{const\ }\hlstd{JacobiSVD}\hlopt{$<$}\hlstd{MatrixXd}\hlopt{$>$}\hspace*{\fill}\\
    \hlstd{}\hlstd{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ }\hlstd{}\hlkwd{UDV}\hlstd{}\hlopt{(}\hlstd{X}\hlopt{.}\hlstd{}\hlkwd{jacobiSvd}\hlstd{}\hlopt{(}\hlstd{Eigen}\hlopt{::}\hlstd{ComputeThinU}\hlopt{\textbar }\hlstd{Eigen}\hlopt{::}\hlstd{ComputeThinV}\hlopt{));}\hspace*{\fill}\\
    \hlstd{}\hlkwb{const\ }\hlstd{ArrayXd}\hlstd{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ }\hlstd{}\hlkwd{D}\hlstd{}\hlopt{(}\hlstd{UDV}\hlopt{.}\hlstd{}\hlkwd{singularValues}\hlstd{}\hlopt{());}\hspace*{\fill}\\
    \hlstd{}\hlkwb{const\ int}\hlstd{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ }\hlkwb{}\hlstd{}\hlkwd{r}\hlstd{}\hlopt{((}\hlstd{D\ }\hlopt{$>$\ }\hlstd{D}\hlopt{{[}}\hlstd{}\hlnum{0}\hlstd{}\hlopt{{]}\ {*}\ }\hlstd{}\hlkwd{threshold}\hlstd{}\hlopt{()).}\hlstd{}\hlkwd{count}\hlstd{}\hlopt{());}\hspace*{\fill}\\
    \hlstd{}\hlkwb{const\ }\hlstd{MatrixXd}\hlstd{\ \ \ \ \ \ \ \ \ \ \ \ }\hlstd{}\hlkwd{VDp}\hlstd{}\hlopt{(}\hlstd{UDV}\hlopt{.}\hlstd{}\hlkwd{matrixV}\hlstd{}\hlopt{()\ {*}\ }\hlstd{}\hlkwd{Dplus}\hlstd{}\hlopt{(}\hlstd{D}\hlopt{,\ }\hlstd{r}\hlopt{));}\hspace*{\fill}\\
    \hlstd{}\hlkwb{const\ }\hlstd{VectorXd}\hlstd{\ \ \ \ \ \ \ \ }\hlstd{}\hlkwd{betahat}\hlstd{}\hlopt{(}\hlstd{VDp\ }\hlopt{{*}\ }\hlstd{UDV}\hlopt{.}\hlstd{}\hlkwd{matrixU}\hlstd{}\hlopt{().}\hlstd{}\hlkwd{adjoint}\hlstd{}\hlopt{()\ {*}\ }\hlstd{y}\hlopt{);}\hspace*{\fill}\\
    \hlstd{}\hlkwb{const\ int}\hlstd{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ }\hlkwb{}\hlstd{}\hlkwd{df}\hlstd{}\hlopt{(}\hlstd{n\ }\hlopt{{-}\ }\hlstd{r}\hlopt{);}\hspace*{\fill}\\
    \hlstd{}\hlkwb{const\ }\hlstd{VectorXd}\hlstd{\ \ \ \ \ \ \ \ \ \ \ \ \ }\hlstd{}\hlkwd{se}\hlstd{}\hlopt{(}\hlstd{s\ }\hlopt{{*}\ }\hlstd{VDp}\hlopt{.}\hlstd{}\hlkwd{rowwise}\hlstd{}\hlopt{().}\hlstd{}\hlkwd{norm}\hlstd{}\hlopt{());}\hlstd{}\hspace*{\fill}
    \normalfont
    \normalsize
  \end{quote}
  \caption{\textbf{SVDLSCpp}: Least squares using the SVD}
  \label{SVDLS}
\end{figure}
% using Eigen::JacobiSVD;

% const JacobiSVD<MatrixXd>
%                 UDV(X.jacobiSvd(Eigen::ComputeThinU|Eigen::ComputeThinV));
% const ArrayXd               D(UDV.singularValues());
% const int                   r((D > D[0] * threshold()).count());
% const MatrixXd            VDp(UDV.matrixV() * Dplus(D, r));
% const VectorXd        betahat(VDp * UDV.matrixU().adjoint() * y);
% const int                  df(n - r);
% const VectorXd             se(s * VDp.rowwise().norm());
In the rank-deficient case this code will produce a complete set of
coefficients and their standard errors.  It is up to the user to note
that the rank is less than $p$, the number of columns in $\bm X$, and
hence that the estimated coefficients are just one of an infinite
number of coefficient vectors that produce the same fitted values.  It
happens that this solution is the minimum norm solution.

The standard errors of the coefficient estimates in the rank-deficient
case must be interpreted carefully.  The solution with one or more missing
coefficients, as returned by the \code{lm.fit} function in
\proglang{R} and by the column-pivoted QR decomposition described in
Section~\ref{sec:colPivQR}, does not provide standard errors for the
missing coefficients.  That is, both the coefficient and its standard
error are returned as \code{NA} because the least squares solution is
performed on a reduced model matrix.  It is also true that the
solution returned by the SVD method is with respect to a reduced model
matrix but the $p$ coefficient estimates and their $p$ standard errors
don't show this.  They are, in fact, linear combinations of a set of
$r$ coefficient estimates and their standard errors.

\subsection{Least squares using the eigendecomposition}
\label{sec:eigendecomp}

The eigendecomposition of $\bm X^\prime\bm X$ is defined as
\begin{displaymath}
  \bm X^\prime\bm X=\bm V\bm\Lambda\bm V^\prime
\end{displaymath}
where $\bm V$, the matrix of eigenvectors, is a $p\times p$ orthogonal
matrix and $\bm\Lambda$ is a $p\times p$ diagonal matrix with
non-increasing, non-negative diagonal elements, called the eigenvalues
of $\bm X^\prime\bm X$.  When the eigenvalues are distinct this $\bm
V$ is the same as that in the SVD.  Also, the eigenvalues of $\bm
X^\prime\bm X$ are the squares of the singular values of $\bm X$.

With these definitions one can adapt much of the code from the SVD
method for the eigendecomposition, as shown in Figure~\ref{SymmEigLS}.
\begin{figure}[htb]
  \begin{quote}
    \noindent
    \ttfamily
    \hlstd{}\hlkwa{using\ }\hlstd{Eigen}\hlopt{::}\hlstd{SelfAdjointEigenSolver}\hlopt{;}\hspace*{\fill}\\
    \hlstd{}\hspace*{\fill}\\
    \hlkwb{const\ }\hlstd{SelfAdjointEigenSolver}\hlopt{$<$}\hlstd{MatrixXd}\hlopt{$>$}\hspace*{\fill}\\
    \hlstd{}\hlstd{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ }\hlstd{}\hlkwd{VLV}\hlstd{}\hlopt{(}\hlstd{}\hlkwd{MatrixXd}\hlstd{}\hlopt{(}\hlstd{p}\hlopt{,\ }\hlstd{p}\hlopt{).}\hlstd{}\hlkwd{setZero}\hlstd{}\hlopt{()}\hspace*{\fill}\\
    \hlstd{}\hlstd{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ }\hlstd{}\hlopt{.}\hlstd{selfadjointView}\hlopt{$<$}\hlstd{Lower}\hlopt{$>$}\hspace*{\fill}\\
    \hlstd{}\hlstd{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ }\hlstd{}\hlopt{.}\hlstd{}\hlkwd{rankUpdate}\hlstd{}\hlopt{(}\hlstd{X}\hlopt{.}\hlstd{}\hlkwd{adjoint}\hlstd{}\hlopt{()));}\hspace*{\fill}\\
    \hlstd{}\hlkwb{const\ }\hlstd{ArrayXd}\hlstd{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ }\hlstd{}\hlkwd{D}\hlstd{}\hlopt{(}\hlstd{eig}\hlopt{.}\hlstd{}\hlkwd{eigenvalues}\hlstd{}\hlopt{());}\hspace*{\fill}\\
    \hlstd{}\hlkwb{const\ int}\hlstd{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ }\hlkwb{}\hlstd{}\hlkwd{r}\hlstd{}\hlopt{((}\hlstd{D\ }\hlopt{$>$\ }\hlstd{D}\hlopt{{[}}\hlstd{p\ }\hlopt{{-}\ }\hlstd{}\hlnum{1}\hlstd{}\hlopt{{]}\ {*}\ }\hlstd{}\hlkwd{threshold}\hlstd{}\hlopt{()).}\hlstd{}\hlkwd{count}\hlstd{}\hlopt{());}\hspace*{\fill}\\
    \hlstd{}\hlkwb{const\ }\hlstd{MatrixXd}\hlstd{\ \ \ \ \ \ \ \ \ \ \ \ }\hlstd{}\hlkwd{VDp}\hlstd{}\hlopt{(}\hlstd{VLV}\hlopt{.}\hlstd{}\hlkwd{eigenvectors}\hlstd{}\hlopt{()\ {*}\ }\hlstd{}\hlkwd{Dplus}\hlstd{}\hlopt{(}\hlstd{D}\hlopt{.}\hlstd{}\hlkwd{sqrt}\hlstd{}\hlopt{(),}\hlstd{r}\hlopt{,}\hlstd{}\hlkwa{true}\hlstd{}\hlopt{));}\hspace*{\fill}\\
    \hlstd{}\hlkwb{const\ }\hlstd{VectorXd}\hlstd{\ \ \ \ \ \ \ \ }\hlstd{}\hlkwd{betahat}\hlstd{}\hlopt{(}\hlstd{VDp\ }\hlopt{{*}\ }\hlstd{VDp}\hlopt{.}\hlstd{}\hlkwd{adjoint}\hlstd{}\hlopt{()\ {*}\ }\hlstd{X}\hlopt{.}\hlstd{}\hlkwd{adjoint}\hlstd{}\hlopt{()\ {*}\ }\hlstd{y}\hlopt{);}\hspace*{\fill}\\
    \hlstd{}\hlkwb{const\ }\hlstd{VectorXd}\hlstd{\ \ \ \ \ \ \ \ \ \ \ \ \ }\hlstd{}\hlkwd{se}\hlstd{}\hlopt{(}\hlstd{s\ }\hlopt{{*}\ }\hlstd{VDp}\hlopt{.}\hlstd{}\hlkwd{rowwise}\hlstd{}\hlopt{().}\hlstd{}\hlkwd{norm}\hlstd{}\hlopt{());}\hlstd{}\hspace*{\fill}
    \normalfont
    \normalsize
  \end{quote}
  \caption{\textbf{SymmEigLSCpp}: Least squares using the eigendecomposition}
  \label{SymmEigLS}
\end{figure}
% using Eigen::SelfAdjointEigenSolver;

% const SelfAdjointEigenSolver<MatrixXd>
%                           VLV(MatrixXd(p, p).setZero()
%                                             .selfadjointView<Lower>
%                                             .rankUpdate(X.adjoint()));
% const ArrayXd               D(eig.eigenvalues());
% const int                   r((D > D[p - 1] * threshold()).count());
% const MatrixXd            VDp(VLV.eigenvectors() * Dplus(D.sqrt(),r,true));
% const VectorXd        betahat(VDp * VDp.adjoint() * X.adjoint() * y);
% const VectorXd             se(s * VDp.rowwise().norm());

\subsection{Least squares using the column-pivoted QR decomposition}
\label{sec:colPivQR}

The column-pivoted QR decomposition provides results similar to those
from \proglang{R} in both the full-rank and the rank-deficient cases.
The decomposition is of the form
\begin{displaymath}
  \bm X\bm P=\bm Q\bm R=\bm Q_1\bm R_1
\end{displaymath}
where, as before, $\bm Q$ is $n\times n$ and orthogonal and $\bm R$ is
$n\times p$ and upper triangular.  The $p\times p$ matrix $\bm P$ is a
permutation matrix.  That is, its columns are a permutation of the
columns of $\bm I_p$.  It serves to reorder the columns of $\bm X$ so
that the diagonal elements of $\bm R$ are non-increasing in magnitude.

An instance of the class \code{Eigen::ColPivHouseholderQR} has a
\code{rank()} method returning the computational rank of the matrix.
When $\bm X$ is of full rank one can use essentially the same code as
in the unpivoted decomposition except that one must reorder the
standard errors.  When $\bm X$ is rank-deficient, the
coefficients and standard errors are evaluated for the leading $r$ columns of $\bm
X\bm P$ only.

In the rank-deficient case the straightforward calculation of the
fitted values, as $\bm X\widehat{\bm\beta}$, cannot be used.  One
could do some complicated rearrangement of the columns of X and the
coefficient estimates but it is conceptually (and computationally)
easier to employ the relationship
\begin{displaymath}
  \widehat{\bm y} = \bm Q_1\bm Q_1^\prime\bm y=\bm Q
  \begin{bmatrix}
    \bm I_r & \bm 0\\
    \bm 0   & \bm 0
  \end{bmatrix}
  \bm Q^\prime\bm y
\end{displaymath}
The vector $\bm Q^\prime\bm y$ is called the ``effects'' vector in \proglang{R}.
\begin{figure}[htb]
  \begin{quote}
    \noindent
    \ttfamily
    \hlstd{}\hlkwa{using\ }\hlstd{Eigen}\hlopt{::}\hlstd{ColPivHouseholderQR}\hlopt{;}\hspace*{\fill}\\
    \hlstd{}\hlkwc{typedef\ }\hlstd{ColPivHouseholderQR}\hlopt{$<$}\hlstd{MatrixXd}\hlopt{$>$::}\hlstd{PermutationType}\hlstd{\ \ }\hlstd{Permutation}\hlopt{;}\hspace*{\fill}\\
    \hlstd{}\hspace*{\fill}\\
    \hlkwb{const\ }\hlstd{ColPivHouseholderQR}\hlopt{$<$}\hlstd{MatrixXd}\hlopt{$>$\ }\hlstd{}\hlkwd{PQR}\hlstd{}\hlopt{(}\hlstd{X}\hlopt{);}\hspace*{\fill}\\
    \hlstd{}\hlkwb{const\ }\hlstd{Permutation}\hlstd{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ }\hlstd{}\hlkwd{Pmat}\hlstd{}\hlopt{(}\hlstd{PQR}\hlopt{.}\hlstd{}\hlkwd{colsPermutation}\hlstd{}\hlopt{());}\hspace*{\fill}\\
    \hlstd{}\hlkwb{const\ int}\hlstd{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ }\hlkwb{}\hlstd{}\hlkwd{r}\hlstd{}\hlopt{(}\hlstd{PQR}\hlopt{.}\hlstd{}\hlkwd{rank}\hlstd{}\hlopt{());}\hspace*{\fill}\\
    \hlstd{VectorXd}\hlstd{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ }\hlstd{betahat}\hlopt{,\ }\hlstd{fitted}\hlopt{,\ }\hlstd{se}\hlopt{;}\hspace*{\fill}\\
    \hlstd{}\hlkwa{if\ }\hlstd{}\hlopt{(}\hlstd{r\ }\hlopt{==\ }\hlstd{X}\hlopt{.}\hlstd{}\hlkwd{cols}\hlstd{}\hlopt{())\ \{\ }\hlstd{}\hlslc{//\ full\ rank\ case}\hspace*{\fill}\\
    \hlstd{}\hlstd{\ \ \ \ }\hlstd{betahat}\hlstd{\ \ }\hlstd{}\hlopt{=\ }\hlstd{PQR}\hlopt{.}\hlstd{}\hlkwd{solve}\hlstd{}\hlopt{(}\hlstd{y}\hlopt{);}\hspace*{\fill}\\
    \hlstd{}\hlstd{\ \ \ \ }\hlstd{fitted}\hlstd{\ \ \ }\hlstd{}\hlopt{=\ }\hlstd{X\ }\hlopt{{*}\ }\hlstd{betahat}\hlopt{;}\hspace*{\fill}\\
    \hlstd{}\hlstd{\ \ \ \ }\hlstd{se}\hlstd{\ \ \ \ \ \ \ }\hlstd{}\hlopt{=\ }\hlstd{Pmat\ }\hlopt{{*}\ }\hlstd{PQR}\hlopt{.}\hlstd{}\hlkwd{matrixQR}\hlstd{}\hlopt{().}\hlstd{}\hlkwd{topRows}\hlstd{}\hlopt{(}\hlstd{p}\hlopt{).}\hlstd{triangularView}\hlopt{$<$}\hlstd{Upper}\hlopt{$>$().}\hspace*{\fill}\\
    \hlstd{}\hlstd{\ \ \ \ \ \ \ \ }\hlstd{}\hlkwd{solve}\hlstd{}\hlopt{(}\hlstd{MatrixXd}\hlopt{::}\hlstd{}\hlkwd{Identity}\hlstd{}\hlopt{(}\hlstd{p}\hlopt{,\ }\hlstd{p}\hlopt{)).}\hlstd{}\hlkwd{rowwise}\hlstd{}\hlopt{().}\hlstd{}\hlkwd{norm}\hlstd{}\hlopt{();}\hspace*{\fill}\\
    \hlstd{}\hlopt{\}\ }\hlstd{}\hlkwa{else\ }\hlstd{}\hlopt{\{}\hspace*{\fill}\\
    \hlstd{}\hlstd{\ \ \ \ }\hlstd{MatrixXd}\hlstd{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ }\hlstd{}\hlkwd{Rinv}\hlstd{}\hlopt{(}\hlstd{PQR}\hlopt{.}\hlstd{}\hlkwd{matrixQR}\hlstd{}\hlopt{().}\hlstd{}\hlkwd{topLeftCorner}\hlstd{}\hlopt{(}\hlstd{r}\hlopt{,\ }\hlstd{r}\hlopt{).}\hspace*{\fill}\\
    \hlstd{}\hlstd{\ \ \ \ \ \ \ \ \ \ \ }\hlstd{triangularView}\hlopt{$<$}\hlstd{Upper}\hlopt{$>$().}\hspace*{\fill}\\
    \hlstd{}\hlstd{\ \ \ \ \ \ \ \ \ \ \ }\hlstd{}\hlkwd{solve}\hlstd{}\hlopt{(}\hlstd{MatrixXd}\hlopt{::}\hlstd{}\hlkwd{Identity}\hlstd{}\hlopt{(}\hlstd{r}\hlopt{,\ }\hlstd{r}\hlopt{)));}\hspace*{\fill}\\
    \hlstd{}\hlstd{\ \ \ \ }\hlstd{VectorXd}\hlstd{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ }\hlstd{}\hlkwd{effects}\hlstd{}\hlopt{(}\hlstd{PQR}\hlopt{.}\hlstd{}\hlkwd{householderQ}\hlstd{}\hlopt{().}\hlstd{}\hlkwd{adjoint}\hlstd{}\hlopt{()\ {*}\ }\hlstd{y}\hlopt{);}\hspace*{\fill}\\
    \hlstd{}\hlstd{\ \ \ \ }\hlstd{betahat}\hlopt{.}\hlstd{}\hlkwd{fill}\hlstd{}\hlopt{(::}\hlstd{NA\textunderscore REAL}\hlopt{);}\hspace*{\fill}\\
    \hlstd{}\hlstd{\ \ \ \ }\hlstd{betahat}\hlopt{.}\hlstd{}\hlkwd{head}\hlstd{}\hlopt{(}\hlstd{r}\hlopt{)}\hlstd{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ }\hlopt{=\ }\hlstd{Rinv\ }\hlopt{{*}\ }\hlstd{effects}\hlopt{.}\hlstd{}\hlkwd{head}\hlstd{}\hlopt{(}\hlstd{r}\hlopt{);}\hspace*{\fill}\\
    \hlstd{}\hlstd{\ \ \ \ }\hlstd{betahat}\hlstd{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ }\hlstd{}\hlopt{=\ }\hlstd{Pmat\ }\hlopt{{*}\ }\hlstd{betahat}\hlopt{;}\hspace*{\fill}\\
    \hlstd{}\hlstd{\ \ \ }\hlstd{}\hlslc{//\ create\ fitted\ values\ from\ effects}\hspace*{\fill}\\
    \hlstd{}\hlstd{\ \ \ }\hlstd{}\hlslc{//\ (cannot\ use\ X\ {*}\ betahat\ when\ X\ is\ rank{-}deficient)}\hspace*{\fill}\\
    \hlstd{}\hlstd{\ \ \ \ }\hlstd{effects}\hlopt{.}\hlstd{}\hlkwd{tail}\hlstd{}\hlopt{(}\hlstd{X}\hlopt{.}\hlstd{}\hlkwd{rows}\hlstd{}\hlopt{()\ {-}\ }\hlstd{r}\hlopt{).}\hlstd{}\hlkwd{setZero}\hlstd{}\hlopt{();}\hspace*{\fill}\\
    \hlstd{}\hlstd{\ \ \ \ }\hlstd{fitted}\hlstd{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ }\hlstd{}\hlopt{=\ }\hlstd{PQR}\hlopt{.}\hlstd{}\hlkwd{householderQ}\hlstd{}\hlopt{()\ {*}\ }\hlstd{effects}\hlopt{;}\hspace*{\fill}\\
    \hlstd{}\hlstd{\ \ \ \ }\hlstd{se}\hlopt{.}\hlstd{}\hlkwd{fill}\hlstd{}\hlopt{(::}\hlstd{NA\textunderscore REAL}\hlopt{);}\hspace*{\fill}\\
    \hlstd{}\hlstd{\ \ \ \ }\hlstd{se}\hlopt{.}\hlstd{}\hlkwd{head}\hlstd{}\hlopt{(}\hlstd{r}\hlopt{)}\hlstd{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ }\hlopt{=\ }\hlstd{Rinv}\hlopt{.}\hlstd{}\hlkwd{rowwise}\hlstd{}\hlopt{().}\hlstd{}\hlkwd{norm}\hlstd{}\hlopt{();}\hspace*{\fill}\\
    \hlstd{}\hlstd{\ \ \ \ }\hlstd{se}\hlstd{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ }\hlstd{}\hlopt{=\ }\hlstd{Pmat\ }\hlopt{{*}\ }\hlstd{se}\hlopt{;}\hspace*{\fill}\\
    \hlstd{}\hlopt{\}}\hlstd{}\hspace*{\fill}
    \normalfont
    \normalsize
  \end{quote}
  \caption{\textbf{ColPivQRLSCpp}: Least squares using the pivoted QR decomposition}
  \label{ColPivQRLS}
\end{figure}
% using Eigen::ColPivHouseholderQR;
% typedef ColPivHouseholderQR<MatrixXd>::PermutationType  Permutation;

% const ColPivHouseholderQR<MatrixXd> PQR(X);
% const Permutation                   Pmat(PQR.colsPermutation());
% const int                              r(PQR.rank());
% VectorXd                               betahat, fitted, se;
% if (r == X.cols()) {	// full rank case
%     betahat  = PQR.solve(y);
%     fitted   = X * betahat;
%     se       = Pmat * PQR.matrixQR().topRows(p).triangularView<Upper>().
% 	       solve(MatrixXd::Identity(p, p)).rowwise().norm();
% } else {
%     MatrixXd                      Rinv(PQR.matrixQR().topLeftCorner(r, r).
% 				       triangularView<Upper>().
% 				       solve(MatrixXd::Identity(r, r)));
%     VectorXd                   effects(PQR.householderQ().adjoint() * y);
%     betahat.fill(::NA_REAL);
%     betahat.head(r)                    = Rinv * effects.head(r);
%     betahat                            = Pmat * betahat;
% 			// create fitted values from effects
% 			// (cannot use X * betahat when X is rank-deficient)
%     effects.tail(X.rows() - r).setZero();
%     fitted                             = PQR.householderQ() * effects;
%     se.fill(::NA_REAL);
%     se.head(r)                         = Rinv.rowwise().norm();
%     se                                 = Pmat * se;
% }

Just to check that the code in Figure~\ref{ColPivQRLS} does indeed provide the desired answer
<<rankdeficientPQR>>=
print(summary(fmPQR <- fastLm(y ~ f1 * f2, dd)), signif.stars=FALSE)
all.equal(coef(fm1), coef(fmPQR))
all.equal(unname(fitted(fm1)), fitted(fmPQR))
all.equal(unname(residuals(fm1)), residuals(fmPQR))
@

The rank-revealing SVD method produces the same fitted
values but not the same coefficients.
<<rankdeficientSVD>>=
print(summary(fmSVD <- fastLm(y ~ f1 * f2, dd, method=4L)), signif.stars=FALSE)
all.equal(coef(fm1), coef(fmSVD))
all.equal(unname(fitted(fm1)), fitted(fmSVD))
all.equal(unname(residuals(fm1)), residuals(fmSVD))
@
The coefficients from the symmetric eigendecomposition method are the same as those from the SVD
<<rankdeficientVLV>>=
print(summary(fmVLV <- fastLm(y ~ f1 * f2, dd, method=5L)), signif.stars=FALSE)
all.equal(coef(fmSVD), coef(fmVLV))
all.equal(unname(fitted(fm1)), fitted(fmSVD))
all.equal(unname(residuals(fm1)), residuals(fmSVD))
@

\subsection{Comparative speed}

In the \pkg{RcppEigen} package the \proglang{R} function to fit linear
models using the methods described above is called \code{fastLm}. It follows
an earlier example in the \pkg{Rcpp} package which was carried over to both
\pkg{RcppArmadillo} and \pkg{RcppGSL}. The natural question to ask is, ``Is it indeed fast to use these methods
based on \pkg{Eigen}?''.  To this end, the example provides benchmarking code for these
methods, \proglang{R}'s \code{lm.fit} function and the \code{fastLm}
implementations in the \pkg{RcppArmadillo} \citep{CRAN:RcppArmadillo}
and \pkg{RcppGSL} \citep{CRAN:RcppGSL} packages, if they are
installed.  The benchmark code, which uses the \pkg{rbenchmark}
\citep{CRAN:rbenchmark} package, is in a file named
\code{lmBenchmark.R} in the \code{examples} subdirectory of the
installed \pkg{RcppEigen} package.


It can be run as
<<benchmark,eval=FALSE>>=
source(system.file("examples", "lmBenchmark.R", package="RcppEigen"))
@
Results will vary according to the speed of the processor, the
number of cores and the implementation of the BLAS (Basic Linear
Algebra Subroutines) used.  (\pkg{Eigen} methods do not use the BLAS
but the other methods do.)  \marginpar{If Eigen uses multiple cores, should
  we not use Goto or another multicore BLAS as well?}

Results obtained on a desktop computer, circa 2010, are shown in
Table~\ref{tab:lmRes}
\begin{table}[tb]
  \caption{\code{lmBenchmark} results on a desktop computer for the
    default size, $100,000\times 40$, full-rank model matrix running
    20 repetitions for each method.  Times (Elapsed, User and Sys) are
    in seconds.  The BLAS in use is a single-threaded version of Atlas
    (Automatically Tuned Linear Algebra System).}
  \label{tab:lmRes}
  \centering
  \begin{tabular}{r r r r r}
    \toprule
    \multicolumn{1}{c}{Method} & \multicolumn{1}{c}{Relative} &
    \multicolumn{1}{c}{Elapsed} & \multicolumn{1}{c}{User} &
    \multicolumn{1}{c}{Sys}\\
    \cmidrule(r){2-5}   % middle rule from cols 2 to 5
     LLt &   1.000000 &   1.227 &     1.228 &    0.000 \\
    LDLt &   1.037490 &   1.273 &     1.272 &    0.000 \\
 SymmEig &   2.895681 &   3.553 &     2.972 &    0.572 \\
      QR &   7.828036 &   9.605 &     8.968 &    0.620 \\
   PivQR &   7.953545 &   9.759 &     9.120 &    0.624 \\
    arma &   8.383048 &  10.286 &    10.277 &    0.000 \\
  lm.fit &  13.782396 &  16.911 &    15.521 &    1.368 \\
     SVD &  54.829666 &  67.276 &    66.321 &    0.912 \\
     GSL & 157.531377 & 193.291 &   192.568 &    0.640 \\
     \bottomrule
  \end{tabular}
\end{table}

These results indicate that methods based on forming and decomposing
$\bm X^\prime\bm X$, (i.e. LDLt, LLt and SymmEig) are considerably
faster than the others.  The SymmEig method, using a rank-revealing
decomposition, would be preferred, although the LDLt method could
probably be modified to be rank-revealing.  Do bear in mind that the
dimensions of the problem will influence the comparative results.
Because there are 100,000 rows in $\bm X$, methods that decompose the
whole $\bm X$ matrix (all the methods except those named above) will
be at a disadvantage.

The pivoted QR method is 1.6 times faster than R's \Sexpr{link("lm.fit")} on
this test and provides nearly the same information as \Sexpr{link("lm.fit")}.
Methods based on the singular value decomposition (SVD and GSL) are
much slower but, as mentioned above, this is caused in part by $\bm X$
having many more rows than columns.  The GSL method from the GNU
Scientific Library uses an older algorithm for the SVD and is clearly
out of contention.

An SVD method using the Lapack SVD subroutine, \code{dgesv}, may be
faster than the native \pkg{Eigen} implementation of the SVD, which is
not a particularly fast method of evaluating the SVD.

\section{Delayed evaluation}
\label{sec:delayed}

A form of delayed evaluation is used in \pkg{Eigen}.  That is, many
operators and methods do not evaluate the result but instead return an
``expression object'' that is evaluated when needed.  As an example,
even though one writes the $\bm X^\prime\bm X$ evaluation using
\code{.rankUpdate(X.adjoint())} the \code{X.adjoint()} part is not
evaluated immediately.  The \code{rankUpdate} method detects that it
has been passed a matrix that is to be used in its transposed form and
evaluates the update by taking inner products of columns of $\bm X$
instead of rows of $\bm X^\prime$.

Occasionally the method for \code{Rcpp::wrap} will not force an
evaluation when it should.  This is at least what Bill Venables calls
an ``infelicity'' in the code, if not an outright bug.  In the code
for the transpose of an integer matrix shown in Figure~\ref{trans} we
assigned the transpose as a \code{MatrixXi} before returning it with
\code{wrap}.  The assignment forces the evaluation.  If this
step is skipped, as in Figure~\ref{badtrans}, an answer with the correct
shape but incorrect contents is obtained.

<<badtransCpp,echo=FALSE>>=
badtransCpp <- '
using Eigen::Map;
using Eigen::MatrixXi;
const Map<MatrixXi>  A(as<Map<MatrixXi> >(AA));
return wrap(A.transpose());
'
@
\begin{figure}[htb]
  \begin{quote}
    \noindent
    \ttfamily
    \hlstd{}\hlkwa{using\ }\hlstd{Eigen}\hlopt{::}\hlstd{Map}\hlopt{;}\hspace*{\fill}\\
    \hlstd{}\hlkwa{using\ }\hlstd{Eigen}\hlopt{::}\hlstd{MatrixXi}\hlopt{;}\hspace*{\fill}\\
    \hlstd{}\hlkwb{const\ }\hlstd{Map}\hlopt{$<$}\hlstd{MatrixXi}\hlopt{$>$}\hlstd{\ \ }\hlopt{}\hlstd{}\hlkwd{A}\hlstd{}\hlopt{(}\hlstd{as}\hlopt{$<$}\hlstd{Map}\hlopt{$<$}\hlstd{MatrixXi}\hlopt{$>$\ $>$(}\hlstd{AA}\hlopt{));}\hspace*{\fill}\\
    \hlstd{}\hlkwa{return\ }\hlstd{}\hlkwd{wrap}\hlstd{}\hlopt{(}\hlstd{A}\hlopt{.}\hlstd{}\hlkwd{transpose}\hlstd{}\hlopt{());}\hlstd{}\hspace*{\fill}
    \normalfont
    \normalsize
  \end{quote}
  \caption{\textbf{badtransCpp}: Transpose producing incorrect results}
  \label{badtrans}
\end{figure}
<<ftrans2>>=
Ai <- matrix(1:6, ncol=2L)
ftrans2 <- cxxfunction(signature(AA = "matrix"), badtransCpp, "RcppEigen")
(At <- ftrans2(Ai))
all.equal(At, t(Ai))
@

Another recommended practice is to assign objects before wrapping them
for return to \proglang{R}.

\section{Sparse matrices}
\label{sec:sparse}

\pkg{Eigen} provides sparse matrix classes.  An \proglang{R} object of
class \Sexpr{linkS4class("dgCMatrix")} (from the \pkg{Matrix}
\citep{CRAN:Matrix} package) can be mapped as in Figure~\ref{sparseProd}.
<<echo=FALSE>>=
sparseProdCpp <- '
using Eigen::Map;
using Eigen::MappedSparseMatrix;
using Eigen::SparseMatrix;
using Eigen::VectorXd;

const MappedSparseMatrix<double>  A(as<MappedSparseMatrix<double> >(AA));
const Map<VectorXd>               y(as<Map<VectorXd> >(yy));
const SparseMatrix<double>       At(A.adjoint());
return List::create(_["At"]  = At,
                    _["Aty"] = At * y);
'
@
\begin{figure}[htb]
  \begin{quote}
    \noindent
    \ttfamily
    \hlstd{}\hlkwa{using\ }\hlstd{Eigen}\hlopt{::}\hlstd{Map}\hlopt{;}\hspace*{\fill}\\
    \hlstd{}\hlkwa{using\ }\hlstd{Eigen}\hlopt{::}\hlstd{MappedSparseMatrix}\hlopt{;}\hspace*{\fill}\\
    \hlstd{}\hlkwa{using\ }\hlstd{Eigen}\hlopt{::}\hlstd{SparseMatrix}\hlopt{;}\hspace*{\fill}\\
    \hlstd{}\hlkwa{using\ }\hlstd{Eigen}\hlopt{::}\hlstd{VectorXd}\hlopt{;}\hspace*{\fill}\\
    \hlstd{}\hspace*{\fill}\\
    \hlkwb{const\ }\hlstd{MappedSparseMatrix}\hlopt{$<$}\hlstd{}\hlkwb{double}\hlstd{}\hlopt{$>$}\hlstd{\ \ }\hlopt{}\hlstd{}\hlkwd{A}\hlstd{}\hlopt{(}\hlstd{as}\hlopt{$<$}\hlstd{MappedSparseMatrix}\hlopt{$<$}\hlstd{}\hlkwb{double}\hlstd{}\hlopt{$>$\ $>$(}\hlstd{AA}\hlopt{));}\hspace*{\fill}\\
    \hlstd{}\hlkwb{const\ }\hlstd{Map}\hlopt{$<$}\hlstd{VectorXd}\hlopt{$>$}\hlstd{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ }\hlopt{}\hlstd{}\hlkwd{y}\hlstd{}\hlopt{(}\hlstd{as}\hlopt{$<$}\hlstd{Map}\hlopt{$<$}\hlstd{VectorXd}\hlopt{$>$\ $>$(}\hlstd{yy}\hlopt{));}\hspace*{\fill}\\
    \hlstd{}\hlkwb{const\ }\hlstd{SparseMatrix}\hlopt{$<$}\hlstd{}\hlkwb{double}\hlstd{}\hlopt{$>$}\hlstd{\ \ \ \ \ \ \ }\hlopt{}\hlstd{}\hlkwd{At}\hlstd{}\hlopt{(}\hlstd{A}\hlopt{.}\hlstd{}\hlkwd{adjoint}\hlstd{}\hlopt{());}\hspace*{\fill}\\
    \hlstd{}\hlkwa{return\ }\hlstd{List}\hlopt{::}\hlstd{}\hlkwd{create}\hlstd{}\hlopt{(}\hlstd{\textunderscore }\hlopt{{[}}\hlstd{}\hlstr{"At"}\hlstd{}\hlopt{{]}}\hlstd{\ \ }\hlopt{=\ }\hlstd{At}\hlopt{,}\hspace*{\fill}\\
    \hlstd{}\hlstd{\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ }\hlstd{\textunderscore }\hlopt{{[}}\hlstd{}\hlstr{"Aty"}\hlstd{}\hlopt{{]}\ =\ }\hlstd{At\ }\hlopt{{*}\ }\hlstd{y}\hlopt{);}\hlstd{}\hspace*{\fill}
    \normalfont
    \normalsize
    \caption{sparseProdCpp: Transpose and product with sparse matrices}
    \label{sparseProd}
  \end{quote}
\end{figure}
<<>>=
sparse1 <- cxxfunction(signature(AA = "dgCMatrix", yy = "numeric"),
                       sparseProdCpp, "RcppEigen")
data(KNex, package="Matrix")
rr <- sparse1(KNex$mm, KNex$y)
stopifnot(all.equal(rr$At, t(KNex$mm)),
          all.equal(rr$Aty, as.vector(crossprod(KNex$mm, KNex$y))))
@

A sparse Cholesky decomposition is provided in \pkg{Eigen} as the
\code{SimplicialCholesky} class. There are also linkages to the
\pkg{CHOLMOD} code from the \pkg{Matrix} package.  At present, both of
these are regarded as experimental.  \marginpar{Should there be a word about
  lme4Eigen and the speedups?}

\section{Summary}

This paper introduced the \pkg{RcppEigen} package which provides high-level
linear algebra computations as an extension to the \proglang{R} system.
\pkg{RcppEigen} is based on the modern \proglang{C++} library \pkg{Eigen}
which combines extended functionality with excellent performance, and
utilizes \pkg{Rcpp} to interface \proglang{R} with \proglang{C++}.
Several illustrations covered common matrix operations and
several approaches to solving a least squares problem---including an extended
discussion of rank-revealing approaches.  A short example provided
an empirical illustration  for the excellent run-time performance of the
\pkg{RcppEigen} package.


\bibliographystyle{plainnat}
\bibliography{Rcpp}

\end{document}

\subsection{Transpose and adjoint of a complex matrix}
\label{sec:complex}
%% this example exposes a problem in converting Rcomplex* to std::complex<double>
<<echo=FALSE,eval=FALSE>>=
code <- '
using Eigen::Map;
using Eigen::MatrixXcd;
  // Map the complex matrix A_ from R
const Map<MatrixXcd>    A(as<Map<MatrixXcd> >(A_));
return List::create(_["transpose"] = A.transpose(),
                    _["adjoint"]   = A.adjoint());
'
writeLines( code, "code.cpp" )
@
<<echo=FALSE,eval=FALSE,results=tex>>=
ex_highlight( "code.cpp" )
@

<<eval=FALSE>>=
fadj <-
    cxxfunction(signature( A_ = "matrix"),
                paste(readLines( "code.cpp" ), collapse = "\n"),
                plugin = "RcppEigen")
A <- matrix(1:6 + 1i*(6:1), nc=2)
fadj(A)
@
